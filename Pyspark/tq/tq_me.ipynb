{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GEneraal Question\n",
    "'''\n",
    "1. Can you provide an overview of your experience working with PySpark and big data processing?\n",
    "2. What motivated you to specialize in PySpark, and how have you applied it in your previous roles?\n",
    "4. How does PySpark relate to Apache Spark, and what advantages does it offer in distributed data processing?\n",
    "7. Provide examples of PySpark DataFrame operations you frequently use.\n",
    "\n",
    "-- #### PYSPARK SQL #######---\n",
    "1. Describe your experience with PySpark SQL.\n",
    "2.  How do you execute SQL queries on PySpark DataFrames?\n",
    "\n",
    "16. What is broadcasting, and how is it useful in PySpark?\n",
    "17. Provide an example scenario where broadcasting can significantly improve performance.\n",
    "18. Discuss your experience with PySparks MLlib.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BASIC \n",
    "'''\n",
    "5. Describe the difference between a DataFrame and an RDD in PySpark.\n",
    "6. Can you explain transformations and actions in PySpark DataFrames?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Architecture**\n",
    "'''\n",
    "3. Explain the basic architecture of PySpark\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIMIZE \n",
    "'''\n",
    "Q) How do you optimize the performance of PySpark jobs?\n",
    "Q) Can you discuss techniques for handling skewed data in PySpark?\n",
    "Q) Explain how data serialization works in PySpark.\n",
    "Q) Discuss the significance of choosing the right compression codec for your PySpark applications.\n",
    "Q) What is AQE \n",
    "Q) What is dynamic Partition Pruning ?\n",
    "Q) What are the different ways to optimize the performance of a PySpark job?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Problem Solving \n",
    "'''\n",
    "12. How do you deal with missing or null values in PySpark DataFrames?\n",
    "\n",
    "13. Are there any specific strategies or functions you prefer for handling missing data?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TroubleShooting & DEbugging \n",
    "'''\n",
    "20. How do you monitor and troubleshoot PySpark jobs?\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MLIB \n",
    "'''\n",
    "19. Can you give examples of machine learning algorithms youve implemented using PySpark?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Generl \n",
    "q) What are the advantages and disadvantages of using PySpark over traditional ETL tools like Apache Nifi or Informatica?\n",
    "\n",
    "\n",
    "# Basic\n",
    "Q What is the difference between map and flatMap transformations? Provide use cases for each.\n",
    "Q) Explain the differences between DataFrame and RDD in PySpark. When would you choose one over the other?\n",
    "â€¢ Difference between RDD, Dataframe and Dataset\n",
    "â€¢ Architecture of Spark and Hadoop\n",
    "â€¢ Internal Working of Spark\n",
    "\n",
    "#Peromance & optimi \n",
    "Q) Explain the concept of shuffling in PySpark. How can you minimize the impact of shuffling on performance?\n",
    "Q)  How would you handle a skewed dataset in PySpark?\n",
    "18. Explain the role of the Catalyst optimizer in PySpark. How does it impact the execution of your code?\n",
    "19. What is the difference between cache() and persist() in PySpark? In which scenarios would you use each?\n",
    "â€¢ What all optimization techniques you have worked on\n",
    "â€¢ Common Errors faced. How to Identify and resolve\n",
    "â€¢ Broadcast v/s sort-merge join\n",
    "â€¢ Repartition v/s Coalesce\n",
    "â€¢ Reduce by Key v/s Group by Key\n",
    "â€¢ Spark memory allocation\n",
    "\n",
    "\n",
    "\n",
    "## custom \n",
    "ğŸ“Œ How do you implement a custom partitioner in PySpark, and what are the use cases?\n",
    "\n",
    "## real-time \n",
    "ğŸ“Œ How do you handle and manage large datasets that do not fit into memory in PySpark?\n",
    "ğŸ“Œ How can you perform data deduplication in PySpark?\n",
    "\n",
    "## Challenge u have faced in prod\n",
    "ğŸ“Œ Describe a complex PySpark project you have worked on. What were the challenges, and how did you overcome them?\n",
    "\n",
    "### JOINS\n",
    "11. How do you perform joins in PySpark? Explain different types of joins and when to use broadcast joins.\n",
    "\n",
    "12. What are accumulators and broadcast variables? How and when would you use them?\n",
    "13. Explain the groupBy vs. groupByKey in PySpark. What are the performance implications of using one over the other?\n",
    "\n",
    "### spark-streaming\n",
    "14. How do you handle late-arriving data in a PySpark streaming job?\n",
    "15. Explain how checkpointing works in PySpark Streaming. Why and when would you use it?\n",
    "\n",
    "### troubleshooting\n",
    "16. How would you troubleshoot and debug a PySpark job that is failing due to memory issues?\n",
    "25. How do you monitor and maintain a PySpark job in a production environment?\n",
    "\n",
    "\n",
    "### Bes Practise\n",
    "17. What are some best practices for writing a production-ready PySpark application?\n",
    "20. How do you read and write data from/to various sources like HDFS, S3, and JDBC in PySpark?\n",
    "21. How do you perform ETL operations using PySpark? Describe a typical ETL pipeline you have built.\n",
    "22. How would you implement data quality checks in a PySpark ETL pipeline?\n",
    "23. What are window functions in PySpark, and how do you use them? Provide a use case where window functions are necessary.\n",
    "24. Explain the role of PySparkâ€™s SQLContext and HiveContext. When would you use one over the other?\n",
    "\n",
    "\n",
    "\n",
    "# ğ——ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ˜ ğ— ğ—¼ğ—±ğ—²ğ˜€\n",
    "â€¢ Client v/s Cluster mode\n",
    "â€¢ When is Cluster mode preferred and vice versa\n",
    "\n",
    "#ğ—™ğ—¶ğ—¹ğ—² ğ—™ğ—¼ğ—¿ğ—ºğ—®ğ˜ğ˜€\n",
    "â€¢ Why Parquet, How Parquet internally stores data\n",
    "â€¢ What is columnar format\n",
    "â€¢ Parquet vs ORC\n",
    "â€¢ Predicate & Projection Pushdown\n",
    "\n",
    "#Oğ˜ğ—µğ—²ğ—¿ ğ—”ğ—¿ğ—²ğ—®ğ˜€\n",
    "â€¢ Use Spark API to solve a use case\n",
    "â€¢ Word count problem\n",
    "\n",
    "2. What happens once you submit the code, and how does Spark work internally?\n",
    "3. What is driver memory, and when does it spill to disk?\n",
    "4. How does the memory manager work?\n",
    "5. When do you get an OOM (Out Of Memory) exception in the driver node and executor node?\n",
    "6. What is executor memory, how is it distributed, and when does it spill to disk?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
