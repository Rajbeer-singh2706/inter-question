{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q14. What is the function of PySpark's pivot() method? \n",
    "The pivot() method in PySpark is used to rotate/transpose data \n",
    "from one column into many Dataframe columns and back using \n",
    "the unpivot() function (). Pivot() is an aggregation in which the \n",
    "values of one of the grouping columns are transposed into \n",
    "separate columns containing different data. \n",
    "To get started, let's make a PySpark DataFrame. \n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import expr \n",
    "#Create spark session \n",
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), \n",
    "(\"Beans\",1600,\"USA\"), \\ \n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",4\n",
    "00,\"China\"), \\ \n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",\n",
    "4000,\"China\"), \\ \n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Bea\n",
    "ns\",2000,\"Mexico\")] \n",
    "columns= [\"Product\",\"Amount\",\"Country\"] \n",
    "df = spark.createDataFrame(data = data, schema = columns) \n",
    "df.printSchema() \n",
    "df.show(truncate=False) \n",
    "Output- \n",
    "Created by Rahul Pupreja, linkedin.com/in/rahulpupreja \n",
    " \n",
    " \n",
    "To determine the entire amount of each product's exports to \n",
    "each nation, we'll group by Product, pivot by Country, and sum \n",
    "by Amount. \n",
    "pivotDF = \n",
    "df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\") \n",
    "pivotDF.printSchema() \n",
    "pivotDF.show(truncate=False) \n",
    "This will convert the nations from DataFrame rows to columns, \n",
    "resulting in the output seen below. Wherever data is missing, it \n",
    "is assumed to be null by default. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q15. In PySpark, how do you generate broadcast \n",
    "variables? Give an example. \n",
    "Broadcast variables in PySpark are read-only shared variables \n",
    "that are stored and accessible on all nodes in a cluster so that \n",
    "processes may access or use them. Instead of sending this \n",
    "information with each job, PySpark uses efficient broadcast \n",
    "algorithms to distribute broadcast variables among workers, \n",
    "lowering communication costs. \n",
    "The broadcast(v) function of the SparkContext class is used to \n",
    "generate a PySpark Broadcast. This method accepts the \n",
    "broadcast parameter v. \n",
    "Generating broadcast in PySpark Shell: \n",
    "broadcastVariable = sc.broadcast(Array(0, 1, 2, 3)) \n",
    "broadcastVariable.value \n",
    "PySpark RDD Broadcast variable example \n",
    "spark=SparkSession.builder.appName('SparkByExample.com')\n",
    ".getOrCreate() \n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"} \n",
    "Created by Rahul Pupreja, linkedin.com/in/rahulpupreja \n",
    " \n",
    "broadcastStates = spark.sparkContext.broadcast(states) \n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"), \n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"), \n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"), \n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\") \n",
    "  ] \n",
    "rdd = spark.sparkContext.parallelize(data) \n",
    "def state_convert(code): \n",
    "    return broadcastState.value[code] \n",
    "res = rdd.map(lambda a: \n",
    "(a[0],a[1],a[2],state_convert(a{3]))).collect() \n",
    "print(res) \n",
    "PySpark DataFrame Broadcast variable example \n",
    "spark=SparkSession.builder.appName('PySpark broadcast \n",
    "variable').getOrCreate() \n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"} \n",
    "broadcastStates = spark.sparkContext.broadcast(states) \n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"), \n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"), \n",
    "    (\"Robert\",\"William\",\"USA\",\"CA\"), \n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\") \n",
    "  ] \n",
    "Created by Rahul Pupreja, linkedin.com/in/rahulpupreja \n",
    " \n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"] \n",
    "df = spark.createDataFrame(data = data, schema = columns) \n",
    "df.printSchema() \n",
    "df.show(truncate=False) \n",
    "def state_convert(code): \n",
    "    return broadcastState.value[code] \n",
    "res = df.rdd.map(lambda a: \n",
    "(a[0],a[1],a[2],state_convert(a[3]))).toDF(column) \n",
    "res.show(truncate=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. You have a cluster of ten nodes with each node having \n",
    "24 CPU cores. The following code works, but it may crash \n",
    "on huge data sets, or at the very least, it may not take \n",
    "advantage of the cluster's full processing capabilities. \n",
    "Which aspect is the most difficult to alter, and how would \n",
    "you go about doing so? \n",
    "def cal(sparkSession: SparkSession): Unit = { val \n",
    "NumNode = 10 val userActivityRdd: RDD[UserActivity] = \n",
    "readUserActivityData(sparkSession) . \n",
    "repartition(NumNode) val result = userActivityRdd .map(e \n",
    "=> (e.userId, 1L)) . reduceByKey(_ + _) result .take(1000) } \n",
    "The repartition command creates ten partitions regardless of \n",
    "how many of them were loaded. On large datasets, they might \n",
    "get fairly huge, and they'll almost certainly outgrow the RAM \n",
    "allotted to a single executor. \n",
    "Created by Rahul Pupreja, linkedin.com/in/rahulpupreja \n",
    " \n",
    "In addition, each executor can only have one partition. This \n",
    "means that just ten of the 240 executors are engaged (10 \n",
    "nodes with 24 cores, each running one executor). \n",
    "If the number is set exceptionally high, the scheduler's cost in \n",
    "handling the partition grows, lowering performance. It may even \n",
    "exceed the execution time in some circumstances, especially \n",
    "for extremely tiny partitions. \n",
    "The optimal number of partitions is between two and three \n",
    "times the number of executors. In the given scenario, 600 = 10 \n",
    "Ã— 24 x 2.5 divisions would be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Explain the following code and what output it will yield- \n",
    "case class User(uId: Long, uName: String) case class \n",
    "UserActivity(uId: Long, activityTypeId: Int, \n",
    "timestampEpochSec: Long) val LoginActivityTypeId = 0 val \n",
    "LogoutActivityTypeId = 1 private def \n",
    "readUserData(sparkSession: SparkSession): RDD[User] = { \n",
    "sparkSession.sparkContext.parallelize( Array( User(1, \n",
    "\"Doe, John\"), User(2, \"Doe, Jane\"), User(3, \"X, Mr.\")) ) } \n",
    "private def readUserActivityData(sparkSession: \n",
    "SparkSession): RDD[UserActivity] = { \n",
    "sparkSession.sparkContext.parallelize( Array( \n",
    "UserActivity(1, LoginActivityTypeId, 1514764800L), \n",
    "UserActivity(2, LoginActivityTypeId, 1514808000L), \n",
    "UserActivity(1, LogoutActivityTypeId, 1514829600L), \n",
    "UserActivity(1, LoginActivityTypeId, 1514894400L)) ) } def \n",
    "calculate(sparkSession: SparkSession): Unit = { val \n",
    "userRdd: RDD[(Long, User)] = \n",
    "readUserData(sparkSession).map(e => (e.userId, e)) val \n",
    "userActivityRdd: RDD[(Long, UserActivity)] = \n",
    "readUserActivityData(sparkSession).map(e => (e.userId, e)) \n",
    "val result = userRdd .leftOuterJoin(userActivityRdd) \n",
    ".filter(e => e._2._2.isDefined && e._2._2.get.activityTypeId \n",
    "== LoginActivityTypeId) .map(e => (e._2._1.uName, \n",
    "e._2._2.get.timestampEpochSec)) .reduceByKey((a, b) => if \n",
    "(a < b) a else b) result .foreach(e => println(s\"${e._1}: \n",
    "${e._2}\")) } \n",
    "The primary function, calculate, reads two pieces of data. \n",
    "(They are given in this case from a constant inline data \n",
    "Created by Rahul Pupreja, linkedin.com/in/rahulpupreja \n",
    " \n",
    "structure that is transformed to a distributed dataset using \n",
    "parallelize.) Each of them is transformed into a tuple by the \n",
    "map, which consists of a userId and the item itself. To combine \n",
    "the two datasets, the userId is utilised. \n",
    "All users' login actions are filtered out of the combined dataset. \n",
    "The uName and the event timestamp are then combined to \n",
    "make a tuple. \n",
    "This is eventually reduced down to merely the initial login \n",
    "record per user, which is then sent to the console. \n",
    "The following will be the yielded output- \n",
    "Doe, John: 1514764800 \n",
    "Doe, Jane: 1514808000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
