{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "focus on more advanced concepts such as \n",
    "- performance optimization, \n",
    "- complex data transformations, \n",
    "- large-scale data processing,  \n",
    "- architectural decision-making. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. **Optimization of PySpark Jobs**\n",
    "# Q) You are processing a large dataset in PySpark that involves multiple joins and aggregations. The job is slow and often times out. \n",
    "# What steps would you take to optimize the performance of the PySpark job?\n",
    "'''\n",
    "- Enable **predicate pushdown** when reading data.\n",
    "- Repartition the data intelligently based on join keys.\n",
    "- Use **broadcast joins** when one of the tables is small.\n",
    "- Cache intermediate DataFrames if they are used multiple times.\n",
    "- Avoid using `count()` or `collect()` unless absolutely necessary.\n",
    "- Set proper **shuffle partitions** based on cluster size (`spark.sql.shuffle.partitions`).\n",
    "- Use **filter** and **select** to reduce the size of DataFrames early in the process.\n",
    "\n",
    "'''\n",
    "# Example: Optimizing joins\n",
    "small_df = broadcast(small_df)\n",
    "large_df.join(small_df, 'join_key', 'inner').repartition('join_key').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. **Handling Skewed Data**\n",
    "# Q) You notice that one of the partitioned columns in your data has highly skewed data, leading to slow tasks in your Spark job. \n",
    "# How would you handle data skewness in PySpark?\n",
    "'''\n",
    "- **Salting**: Introduce artificial keys to distribute the data more evenly.\n",
    "- Use **random partitioning** to reduce the imbalance.\n",
    "- **Broadcast join** when the smaller dataset is skewed.\n",
    "- Increase the **shuffle partitions**.\n",
    "\n",
    "'''  \n",
    "from pyspark.sql.functions import expr, monotonically_increasing_id\n",
    "\n",
    "# Example of salting\n",
    "df = df.withColumn(\"salted_key\", expr(\"concat(join_key, '_', (monotonically_increasing_id() % 10))\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. **Data Pipeline Design**\n",
    "#   - **Question:** You are tasked with designing an ETL pipeline to process a daily batch of 1 TB of JSON files stored in S3 and \n",
    "# load the processed data into a Redshift table. Explain the design of your pipeline using PySpark and include optimizations for both \n",
    "# compute and storage.\n",
    "'''\n",
    "     - Use **S3 Select** to filter data while reading large JSON files.\n",
    "     - Set up **dynamic partition pruning** and **predicate pushdown**.\n",
    "     - Use **incremental loads** and watermarking to handle late data.\n",
    "     - Use **DataFrame caching** and repartitioning based on the Redshift schema.\n",
    "     - Compress the data using Parquet or ORC before loading into Redshift for space and performance efficiency.\n",
    "     - Use **copy command** or **AWS Glue** for efficient Redshift load.\n",
    "''' \n",
    "df = spark.read.json('s3://bucket/data/')\n",
    "df_filtered = df.filter(df['date'] == '2024-09-13')  # Partition pruning\n",
    "df_filtered.write.parquet('/path/to/save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. **Streaming and Batch Integration**\n",
    "# Explain how you would integrate both batch and real-time streaming data pipelines in PySpark. For example, consider you have to \n",
    "# process clickstream data (streaming) along with daily user profile updates (batch).\n",
    "'''\n",
    "    - Use **Structured Streaming** to handle real-time clickstream data.\n",
    "    - Batch user profile updates are processed separately and joined with the streaming data.\n",
    "    - Utilize **watermarking** to handle late data in the streaming pipeline.\n",
    "    - Output both streams and batch results in the same storage format (e.g., Parquet) for consistency.\n",
    "\n",
    "'''\n",
    "\n",
    "# Example: Streaming and batch join\n",
    "streaming_df = spark.readStream.format(\"kafka\").load()\n",
    "batch_df = spark.read.parquet(\"/path/to/batch\")\n",
    "result_df = streaming_df.join(batch_df, \"user_id\", \"left_outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. **Advanced Window Functions**\n",
    "   - **Question:** You have a dataset that tracks stock prices in real-time, and you need to calculate a rolling 7-day average price for each stock symbol. How would you implement this using PySpark?\n",
    "   - **Expected Solution:**\n",
    "     Use **Window functions** with time-based partitioning.\n",
    "     ```python\n",
    "     from pyspark.sql.window import Window\n",
    "     from pyspark.sql.functions import avg\n",
    "\n",
    "     window_spec = Window.partitionBy('symbol').orderBy('date').rowsBetween(-6, 0)\n",
    "     df.withColumn('7_day_avg_price', avg('price').over(window_spec)).show()\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 6. **Fault Tolerance and Error Handling in PySpark**\n",
    "   - **Question:** Describe how you would handle fault tolerance and ensure data consistency in a large-scale PySpark application that processes real-time sensor data.\n",
    "   - **Expected Solution:**\n",
    "     - Enable **checkpointing** for long-running streaming jobs to ensure fault tolerance.\n",
    "     - Handle **bad records** using `badRecordsPath` option in file-based data ingestion.\n",
    "     - Implement **idempotent** writes using unique identifiers for each record.\n",
    "     - Use **exactly-once semantics** with Structured Streaming and transactional sinks like Delta Lake.\n",
    "     ```python\n",
    "     streaming_df.writeStream \\\n",
    "       .format(\"delta\") \\\n",
    "       .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "       .start(\"/path/to/output\")\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 7. **Handling Large Data with PySpark and Partitioning Strategies**\n",
    "   - **Question:** You have a dataset that exceeds 10 TB, and you need to efficiently store and process it using PySpark. How would you design the storage layout and partitioning strategy to optimize read and write performance?\n",
    "   - **Expected Solution:**\n",
    "     - Partition the data based on high cardinality columns (e.g., `date`, `region`).\n",
    "     - Use **bucketing** on frequently used join columns to reduce shuffle.\n",
    "     - Store the data in **Parquet/ORC** format with snappy compression for efficient reads and writes.\n",
    "     ```python\n",
    "     df.write \\\n",
    "       .partitionBy('date', 'region') \\\n",
    "       .bucketBy(10, 'user_id') \\\n",
    "       .format('parquet') \\\n",
    "       .save('/path/to/save')\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. **Custom UDFs and UDAFs**\n",
    "   - **Question:** Write a custom PySpark UDF that takes a string column and returns the reverse of each string. \n",
    "How would you handle performance issues with UDFs?\n",
    "   - **Expected Solution:**\n",
    "     - Use **pandas UDFs** (vectorized UDFs) for performance improvement.\n",
    "     ```python\n",
    "     from pyspark.sql.functions import udf\n",
    "     from pyspark.sql.types import StringType\n",
    "\n",
    "     @udf(returnType=StringType())\n",
    "     def reverse_string(s):\n",
    "         return s[::-1]\n",
    "\n",
    "     df.withColumn('reversed', reverse_string(df['column'])).show()\n",
    "\n",
    "     # Pandas UDF\n",
    "     from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "     @pandas_udf(StringType())\n",
    "     def reverse_string_pandas(s):\n",
    "         return s.apply(lambda x: x[::-1])\n",
    "\n",
    "     df.withColumn('reversed', reverse_string_pandas(df['column'])).show()\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9. **Cluster Resource Management and Tuning**\n",
    "   - **Question:** You are working on a PySpark job in a YARN cluster that frequently runs out of memory. What configurations and tuning steps would you apply to manage memory and resources effectively?\n",
    "   - **Expected Solution:**\n",
    "     - Increase **executor memory** and **driver memory** based on the job's requirements.\n",
    "     - Tune **number of cores** and **executors** to balance parallelism and resource usage.\n",
    "     - Set **spark.memory.fraction** and **spark.memory.storageFraction** to optimize memory for execution and storage.\n",
    "     - Use **dynamic allocation** to optimize resource usage across different stages.\n",
    "     ```python\n",
    "     spark = SparkSession.builder \\\n",
    "       .config(\"spark.executor.memory\", \"8g\") \\\n",
    "       .config(\"spark.executor.cores\", \"4\") \\\n",
    "       .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "       .getOrCreate()\n",
    "     ```\n",
    "\n",
    "These questions require deeper knowledge of PySpark internals, data engineering architecture, performance tuning, and real-world problem-solving."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
