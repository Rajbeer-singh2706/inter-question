{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "focus on more advanced concepts such as \n",
    "- performance optimization, \n",
    "- complex data transformations, \n",
    "- large-scale data processing,  \n",
    "- architectural decision-making. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. **Optimization of PySpark Jobs**\n",
    "# Q) You are processing a large dataset in PySpark that involves multiple joins and aggregations. The job is slow and often times out. \n",
    "# What steps would you take to optimize the performance of the PySpark job?\n",
    "'''\n",
    "- Enable **predicate pushdown** when reading data.\n",
    "- Repartition the data intelligently based on join keys.\n",
    "- Use **broadcast joins** when one of the tables is small.\n",
    "- Cache intermediate DataFrames if they are used multiple times.\n",
    "- Avoid using `count()` or `collect()` unless absolutely necessary.\n",
    "- Set proper **shuffle partitions** based on cluster size (`spark.sql.shuffle.partitions`).\n",
    "- Use **filter** and **select** to reduce the size of DataFrames early in the process.\n",
    "\n",
    "'''\n",
    "# Example: Optimizing joins\n",
    "small_df = broadcast(small_df)\n",
    "large_df.join(small_df, 'join_key', 'inner').repartition('join_key').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. **Handling Skewed Data**\n",
    "# Q) You notice that one of the partitioned columns in your data has highly skewed data, leading to slow tasks in your Spark job. \n",
    "# How would you handle data skewness in PySpark?\n",
    "'''\n",
    "- **Salting**: Introduce artificial keys to distribute the data more evenly.\n",
    "- Use **random partitioning** to reduce the imbalance.\n",
    "- **Broadcast join** when the smaller dataset is skewed.\n",
    "- Increase the **shuffle partitions**.\n",
    "\n",
    "'''  \n",
    "from pyspark.sql.functions import expr, monotonically_increasing_id\n",
    "\n",
    "# Example of salting\n",
    "df = df.withColumn(\"salted_key\", expr(\"concat(join_key, '_', (monotonically_increasing_id() % 10))\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. **Data Pipeline Design**\n",
    "#   - **Question:** You are tasked with designing an ETL pipeline to process a daily batch of 1 TB of JSON files stored in S3 and \n",
    "# load the processed data into a Redshift table. Explain the design of your pipeline using PySpark and include optimizations for both \n",
    "# compute and storage.\n",
    "'''\n",
    "     - Use **S3 Select** to filter data while reading large JSON files.\n",
    "     - Set up **dynamic partition pruning** and **predicate pushdown**.\n",
    "     - Use **incremental loads** and watermarking to handle late data.\n",
    "     - Use **DataFrame caching** and repartitioning based on the Redshift schema.\n",
    "     - Compress the data using Parquet or ORC before loading into Redshift for space and performance efficiency.\n",
    "     - Use **copy command** or **AWS Glue** for efficient Redshift load.\n",
    "''' \n",
    "df = spark.read.json('s3://bucket/data/')\n",
    "df_filtered = df.filter(df['date'] == '2024-09-13')  # Partition pruning\n",
    "df_filtered.write.parquet('/path/to/save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. **Streaming and Batch Integration**\n",
    "# Explain how you would integrate both batch and real-time streaming data pipelines in PySpark. For example, consider you have to \n",
    "# process clickstream data (streaming) along with daily user profile updates (batch).\n",
    "'''\n",
    "    - Use **Structured Streaming** to handle real-time clickstream data.\n",
    "    - Batch user profile updates are processed separately and joined with the streaming data.\n",
    "    - Utilize **watermarking** to handle late data in the streaming pipeline.\n",
    "    - Output both streams and batch results in the same storage format (e.g., Parquet) for consistency.\n",
    "\n",
    "'''\n",
    "\n",
    "# Example: Streaming and batch join\n",
    "streaming_df = spark.readStream.format(\"kafka\").load()\n",
    "batch_df = spark.read.parquet(\"/path/to/batch\")\n",
    "result_df = streaming_df.join(batch_df, \"user_id\", \"left_outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. **Advanced Window Functions**\n",
    "   - **Question:** You have a dataset that tracks stock prices in real-time, and you need to calculate a rolling 7-day average price for each stock symbol. How would you implement this using PySpark?\n",
    "   - **Expected Solution:**\n",
    "     Use **Window functions** with time-based partitioning.\n",
    "     ```python\n",
    "     from pyspark.sql.window import Window\n",
    "     from pyspark.sql.functions import avg\n",
    "\n",
    "     window_spec = Window.partitionBy('symbol').orderBy('date').rowsBetween(-6, 0)\n",
    "     df.withColumn('7_day_avg_price', avg('price').over(window_spec)).show()\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 6. **Fault Tolerance and Error Handling in PySpark**\n",
    "   - **Question:** Describe how you would handle fault tolerance and ensure data consistency in a large-scale PySpark application that processes real-time sensor data.\n",
    "   - **Expected Solution:**\n",
    "     - Enable **checkpointing** for long-running streaming jobs to ensure fault tolerance.\n",
    "     - Handle **bad records** using `badRecordsPath` option in file-based data ingestion.\n",
    "     - Implement **idempotent** writes using unique identifiers for each record.\n",
    "     - Use **exactly-once semantics** with Structured Streaming and transactional sinks like Delta Lake.\n",
    "     ```python\n",
    "     streaming_df.writeStream \\\n",
    "       .format(\"delta\") \\\n",
    "       .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "       .start(\"/path/to/output\")\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 7. **Handling Large Data with PySpark and Partitioning Strategies**\n",
    "   - **Question:** You have a dataset that exceeds 10 TB, and you need to efficiently store and process it using PySpark. How would you design the storage layout and partitioning strategy to optimize read and write performance?\n",
    "   - **Expected Solution:**\n",
    "     - Partition the data based on high cardinality columns (e.g., `date`, `region`).\n",
    "     - Use **bucketing** on frequently used join columns to reduce shuffle.\n",
    "     - Store the data in **Parquet/ORC** format with snappy compression for efficient reads and writes.\n",
    "     ```python\n",
    "     df.write \\\n",
    "       .partitionBy('date', 'region') \\\n",
    "       .bucketBy(10, 'user_id') \\\n",
    "       .format('parquet') \\\n",
    "       .save('/path/to/save')\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. **Custom UDFs and UDAFs**\n",
    "   - **Question:** Write a custom PySpark UDF that takes a string column and returns the reverse of each string. \n",
    "How would you handle performance issues with UDFs?\n",
    "   - **Expected Solution:**\n",
    "     - Use **pandas UDFs** (vectorized UDFs) for performance improvement.\n",
    "     ```python\n",
    "     from pyspark.sql.functions import udf\n",
    "     from pyspark.sql.types import StringType\n",
    "\n",
    "     @udf(returnType=StringType())\n",
    "     def reverse_string(s):\n",
    "         return s[::-1]\n",
    "\n",
    "     df.withColumn('reversed', reverse_string(df['column'])).show()\n",
    "\n",
    "     # Pandas UDF\n",
    "     from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "     @pandas_udf(StringType())\n",
    "     def reverse_string_pandas(s):\n",
    "         return s.apply(lambda x: x[::-1])\n",
    "\n",
    "     df.withColumn('reversed', reverse_string_pandas(df['column'])).show()\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9. **Cluster Resource Management and Tuning**\n",
    "   - **Question:** You are working on a PySpark job in a YARN cluster that frequently runs out of memory. What configurations and tuning steps would you apply to manage memory and resources effectively?\n",
    "   - **Expected Solution:**\n",
    "     - Increase **executor memory** and **driver memory** based on the job's requirements.\n",
    "     - Tune **number of cores** and **executors** to balance parallelism and resource usage.\n",
    "     - Set **spark.memory.fraction** and **spark.memory.storageFraction** to optimize memory for execution and storage.\n",
    "     - Use **dynamic allocation** to optimize resource usage across different stages.\n",
    "     ```python\n",
    "     spark = SparkSession.builder \\\n",
    "       .config(\"spark.executor.memory\", \"8g\") \\\n",
    "       .config(\"spark.executor.cores\", \"4\") \\\n",
    "       .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "       .getOrCreate()\n",
    "     ```\n",
    "\n",
    "These questions require deeper knowledge of PySpark internals, data engineering architecture, performance tuning, and real-world problem-solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are some advanced **PySpark problems** for a **senior data engineer** to solve. These tasks involve performance optimization, data partitioning, window functions, and stream processing. They are designed to test real-world problem-solving skills.\n",
    "\n",
    "### 1. **Large Dataset Join Optimization**\n",
    "   - **Problem:** You have two large datasets: `user_logs` (10 billion rows) and `user_profiles` (10 million rows). The two DataFrames need to be joined on the `user_id` column. The join operation is slow, and you're running into memory issues.\n",
    "     - **Task:**\n",
    "       - Write a PySpark solution to optimize this join.\n",
    "       - Hint: Consider using **broadcast joins** or repartitioning strategies.\n",
    "     ```python\n",
    "     from pyspark.sql.functions import broadcast\n",
    "\n",
    "     # Example: Use broadcast join for the smaller dataset\n",
    "     result = user_logs.join(broadcast(user_profiles), \"user_id\", \"inner\")\n",
    "     result.show()\n",
    "     ```\n",
    "\n",
    "### 2. **Handling Skewed Data in a Join**\n",
    "   - **Problem:** You are working with two datasets that need to be joined. One of the datasets, `sales_data`, is highly skewed, with 90% of the rows having the same value for the `region` column.\n",
    "     - **Task:**\n",
    "       - Write a solution to handle this skew in PySpark.\n",
    "       - Hint: Use techniques like **salting** to distribute the skewed data across multiple partitions.\n",
    "     ```python\n",
    "     from pyspark.sql.functions import expr, monotonically_increasing_id\n",
    "\n",
    "     # Example: Add salt to distribute skewed data\n",
    "     salted_sales = sales_data.withColumn(\"salted_region\", expr(\"concat(region, '_', (monotonically_increasing_id() % 10))\"))\n",
    "     result = salted_sales.join(other_df, salted_sales['salted_region'] == other_df['region'], \"inner\")\n",
    "     result.show()\n",
    "     ```\n",
    "\n",
    "### 3. **Rolling Window Calculation on Time Series Data**\n",
    "   - **Problem:** You are given a dataset `stock_prices` with columns `symbol`, `date`, and `price`. You are required to calculate the 30-day rolling average price for each stock symbol.\n",
    "     - **Task:**\n",
    "       - Write a PySpark solution that computes the rolling average using **Window functions**.\n",
    "     ```python\n",
    "     from pyspark.sql.window import Window\n",
    "     from pyspark.sql.functions import avg\n",
    "\n",
    "     window_spec = Window.partitionBy('symbol').orderBy('date').rowsBetween(-29, 0)\n",
    "     result = stock_prices.withColumn('30_day_avg_price', avg('price').over(window_spec))\n",
    "     result.show()\n",
    "     ```\n",
    "\n",
    "### 4. **Processing Real-Time Streaming Data**\n",
    "   - **Problem:** You are tasked with processing real-time clickstream data from a Kafka source. Each event contains a `user_id`, `timestamp`, and `action`. You need to write a PySpark Structured Streaming job to:\n",
    "     - Filter out `action = 'login'`\n",
    "     - Group the data by `user_id` and calculate the total number of logins per 5-minute window.\n",
    "     - Output the result to a Parquet file.\n",
    "     - **Task:**\n",
    "       - Implement this streaming job using PySpark.\n",
    "     ```python\n",
    "     from pyspark.sql.functions import window, col\n",
    "     from pyspark.sql import SparkSession\n",
    "\n",
    "     spark = SparkSession.builder.appName(\"StreamingApp\").getOrCreate()\n",
    "\n",
    "     # Read from Kafka source\n",
    "     df = spark.readStream.format(\"kafka\") \\\n",
    "       .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "       .option(\"subscribe\", \"clickstream\") \\\n",
    "       .load()\n",
    "\n",
    "     # Parse the value and create a DataFrame\n",
    "     df_parsed = df.selectExpr(\"CAST(value AS STRING)\").alias(\"json\") \\\n",
    "       .selectExpr(\"json['user_id']\", \"json['timestamp']\", \"json['action']\")\n",
    "\n",
    "     # Filter for login actions\n",
    "     df_filtered = df_parsed.filter(col('action') == 'login')\n",
    "\n",
    "     # Group by user and count logins within 5-minute windows\n",
    "     result = df_filtered.groupBy(\n",
    "       window(col(\"timestamp\"), \"5 minutes\"), col(\"user_id\")\n",
    "     ).count()\n",
    "\n",
    "     # Write the result to Parquet\n",
    "     result.writeStream \\\n",
    "       .outputMode(\"append\") \\\n",
    "       .format(\"parquet\") \\\n",
    "       .option(\"path\", \"/path/to/output\") \\\n",
    "       .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "       .start()\n",
    "     ```\n",
    "\n",
    "### 5. **Handling Large Files with Efficient Partitioning**\n",
    "   - **Problem:** You have a large dataset (5 TB of Parquet files) stored in S3, and you are tasked with optimizing the reading process for analytical queries on specific columns like `region` and `date`.\n",
    "     - **Task:**\n",
    "       - Design an efficient PySpark read operation for this large dataset.\n",
    "       - Apply a partitioning strategy to optimize future reads based on `region` and `date`.\n",
    "     ```python\n",
    "     # Read from S3 and partition the dataset by region and date\n",
    "     df = spark.read.parquet(\"s3://bucket/large_dataset/\")\n",
    "     \n",
    "     # Partition the dataset\n",
    "     df.write.partitionBy(\"region\", \"date\").parquet(\"s3://bucket/optimized_dataset/\")\n",
    "     \n",
    "     # Optimize reading by selecting specific partitions\n",
    "     optimized_df = spark.read.parquet(\"s3://bucket/optimized_dataset/\") \\\n",
    "       .filter(\"region = 'US' AND date = '2024-09-13'\")\n",
    "     optimized_df.show()\n",
    "     ```\n",
    "\n",
    "### 6. **ETL Pipeline for Incremental Data**\n",
    "   - **Problem:** You need to build an ETL pipeline that reads daily JSON files from an S3 bucket, processes the data, and writes it to a Redshift table. The pipeline should only process new data (incremental loads).\n",
    "     - **Task:**\n",
    "       - Implement a PySpark solution to read and process the new files and append the result to Redshift.\n",
    "       - Ensure that old data is not reprocessed.\n",
    "     ```python\n",
    "     from pyspark.sql import SparkSession\n",
    "\n",
    "     spark = SparkSession.builder.appName(\"ETL Pipeline\").getOrCreate()\n",
    "\n",
    "     # Read the latest JSON files\n",
    "     df = spark.read.json(\"s3://bucket/daily_data/2024-09-13/\")\n",
    "\n",
    "     # Apply transformations (e.g., filter, aggregation)\n",
    "     transformed_df = df.filter(df['event'] == 'purchase')\n",
    "\n",
    "     # Write to Redshift\n",
    "     transformed_df.write \\\n",
    "       .format(\"jdbc\") \\\n",
    "       .option(\"url\", \"jdbc:redshift://your-redshift-url\") \\\n",
    "       .option(\"dbtable\", \"public.purchases\") \\\n",
    "       .option(\"user\", \"username\") \\\n",
    "       .option(\"password\", \"password\") \\\n",
    "       .mode(\"append\") \\\n",
    "       .save()\n",
    "     ```\n",
    "\n",
    "### 7. **Custom Aggregation with UDAFs**\n",
    "   - **Problem:** You need to write a custom **User Defined Aggregate Function (UDAF)** in PySpark to calculate the median value for a given column in a DataFrame.\n",
    "     - **Task:**\n",
    "       - Write a PySpark solution using a UDAF to compute the median value of a column.\n",
    "     ```python\n",
    "     from pyspark.sql.expressions import UserDefinedAggregateFunction\n",
    "     from pyspark.sql.types import *\n",
    "\n",
    "     class MedianUDAF(UserDefinedAggregateFunction):\n",
    "         def inputSchema(self):\n",
    "             return StructType([StructField(\"input\", DoubleType())])\n",
    "         \n",
    "         def bufferSchema(self):\n",
    "             return StructType([StructField(\"buffer\", ArrayType(DoubleType()))])\n",
    "         \n",
    "         def dataType(self):\n",
    "             return DoubleType()\n",
    "         \n",
    "         def deterministic(self):\n",
    "             return True\n",
    "         \n",
    "         def initialize(self, buffer):\n",
    "             buffer[0] = []\n",
    "         \n",
    "         def update(self, buffer, input):\n",
    "             buffer[0].append(input)\n",
    "         \n",
    "         def merge(self, buffer1, buffer2):\n",
    "             buffer1[0].extend(buffer2[0])\n",
    "         \n",
    "         def evaluate(self, buffer):\n",
    "             sorted_values = sorted(buffer[0])\n",
    "             count = len(sorted_values)\n",
    "             if count % 2 == 0:\n",
    "                 return (sorted_values[count // 2 - 1] + sorted_values[count // 2]) / 2\n",
    "             else:\n",
    "                 return sorted_values[count // 2]\n",
    "\n",
    "     # Register and use UDAF\n",
    "     spark.udf.register(\"median_udaf\", MedianUDAF())\n",
    "\n",
    "     df = spark.createDataFrame([(1.0,), (2.0,), (3.0,), (4.0,), (5.0,)], [\"value\"])\n",
    "     df.selectExpr(\"median_udaf(value)\").show()\n",
    "     ```\n",
    "\n",
    "### 8. **Managing Data Lineage and Metadata**\n",
    "   - **Problem:** You need to ensure full data lineage and metadata tracking for your PySpark ETL pipelines.\n",
    "     - **Task:**\n",
    "       - Write a PySpark solution to track metadata (e.g., processing time, source file, transformation steps) for every job run and store it in a separate metadata table.\n",
    "     ```python\n",
    "     from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "     df = spark.read.csv(\"s3://bucket/data/\")\n",
    "     df = df.withColumn(\"processing_time\", current_timestamp()) \\\n",
    "            .withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "     # Save data and metadata\n",
    "     df.write.parquet(\"/path/to/processed_data\")\n",
    "\n",
    "     metadata_df = df.select(\"processing_time\", \"source_file\").distinct()\n",
    "     metadata_df.write.mode(\"append\").parquet(\"/path/to/metadata\")\n",
    "     ```\n",
    "\n",
    "These problems focus on real-world PySpark challenges, covering performance, partitioning, custom UDFs, streaming, and ETL pipeline designs, allowing you to demonstrate advanced skills in data engineering."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
