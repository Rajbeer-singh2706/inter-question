{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "'''\n",
    "  1. read the csv file \n",
    "  2. perform the transformation\n",
    "  3. do the upsert \n",
    "  4. finally insert into rds table\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's a step-by-step guide to performing these tasks using PySpark:\n",
    "\n",
    "### **1. Read the CSV File**\n",
    "### First, read a CSV file into a PySpark DataFrame.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadCSV\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the CSV file and S3 bucket\n",
    "input_csv_path = \"s3://your-bucket-name/input-folder/input-file.csv\"  # Input CSV file in S3\n",
    "output_s3_path = \"s3://your-bucket-name/output-folder/transformed-data/\"  # Output S3 path\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"path/to/your/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "### **2. Perform the Transformation**\n",
    "#Next, perform any necessary transformations on the DataFrame. For example, you might want to filter out rows \n",
    "#with null values in certain columns and add a new column.\n",
    "from pyspark.sql.functions import col, lit,date_format\n",
    "\n",
    "# Filter out rows with null values in the 'important_column'\n",
    "df_filtered = df.filter(col(\"important_column\").isNotNull())\n",
    "\n",
    "# Add a new column 'new_column' with a constant value\n",
    "df_transformed = df_filtered.withColumn(\"new_column\", lit(\"some_value\"))\n",
    "\n",
    "\n",
    "# Assume the date column is named 'date_column'\n",
    "# Extract date, month, and year from the date column\n",
    "transformed_df = df.withColumn(\"date\", date_format(col(\"date_column\"), \"dd\")) \\\n",
    "                   .withColumn(\"month\", date_format(col(\"date_column\"), \"MM\")) \\\n",
    "                   .withColumn(\"year\", date_format(col(\"date_column\"), \"yyyy\"))\n",
    "\n",
    "#**Explanation:**\n",
    "#- `filter(col(\"important_column\").isNotNull())` filters out rows where `important_column` is null.\n",
    "#- `withColumn(\"new_column\", lit(\"some_value\"))` adds a new column named `new_column` with a constant value.\n",
    "\n",
    "### **3. Perform the Upsert Operation**\n",
    "#To perform an upsert (insert or update) operation, you'll need to match records in your DataFrame against records in the target table in RDS. \n",
    "# Let's assume we have an `id` column that can be used to match records.\n",
    "\n",
    "# Assuming df_target is the DataFrame read from the target RDS table\n",
    "df_target = spark.read.jdbc(url=\"jdbc:mysql://your-rds-endpoint/db_name\",\n",
    "                            table=\"target_table\",\n",
    "                            properties={\"user\": \"your_username\", \"password\": \"your_password\"})\n",
    "\n",
    "# Merge the DataFrames based on the 'id' column\n",
    "df_upsert = df_transformed.alias(\"source\").join(\n",
    "    df_target.alias(\"target\"),\n",
    "    on=[\"id\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# Resolve conflicts and determine updates or inserts\n",
    "df_upsert_final = df_upsert.selectExpr(\n",
    "    \"coalesce(source.id, target.id) as id\",\n",
    "    \"coalesce(source.column1, target.column1) as column1\",\n",
    "    \"coalesce(source.column2, target.column2) as column2\",\n",
    "    # Include other columns as needed\n",
    ")\n",
    "\n",
    "#**Explanation:**\n",
    "#- The `join` operation merges the source DataFrame with the target DataFrame from RDS.\n",
    "#- `coalesce` is used to resolve conflicts between source and target, preferring the non-null value.\n",
    "\n",
    "\n",
    "### **4. Insert into RDS Table**\n",
    "#Finally, insert the upserted DataFrame back into the RDS table.\n",
    "\n",
    "# Write the final DataFrame back to the RDS table\n",
    "df_upsert_final.write.jdbc(\n",
    "    url=\"jdbc:mysql://your-rds-endpoint/db_name\",\n",
    "    table=\"target_table\",\n",
    "    mode=\"overwrite\",  # Use 'append' if you want to add new records without deleting old ones\n",
    "    properties={\"user\": \"your_username\", \"password\": \"your_password\"}\n",
    ")\n",
    "\n",
    "# Save the transformed DataFrame to S3 in CSV format\n",
    "transformed_df.write.mode(\"overwrite\").csv(output_s3_path, header=True)\n",
    "\n",
    "\n",
    "#**Explanation:**\n",
    "#- `mode=\"overwrite\"` replaces the existing table with the new data. Use `mode=\"append\"` to add new records without replacing the old ones.\n",
    "#- The `jdbc` method is used to write the DataFrame to an RDS table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe vs dynamic frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Optimization Techniques\n",
    "'''\n",
    "Question: What are some best practices for optimizing PySpark code?\n",
    " - Use select() to project only necessary columns.\n",
    " - Cache DataFrames when they are reused multiple times using cache() or persist().\n",
    "Avoid using collect() on large datasets.\n",
    "Use broadcast() for small DataFrames when joining with a large DataFrame.\n",
    "Consider using partitioning and bucketing for large datasets\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Window Functions\n",
    "# Question: Can you explain how to use window functions in PySpark to calculate a moving average?\n",
    "# You can use the window function to define the partitioning and ordering, and then apply aggregation functions like avg() over this window.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "window_spec = Window.partitionBy(\"partition_col\").orderBy(\"order_col\").rowsBetween(-2, 0)\n",
    "df = df.withColumn(\"moving_avg\", avg(\"value_col\").over(window_spec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. UDFs (User Defined Functions)\n",
    "# Question: How would you create and use a UDF in PySpark to apply a custom function on a DataFrame column?\n",
    "# Answer: First, define the UDF using the udf decorator or function, and then apply it to the DataFrame column.\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def custom_function(value):\n",
    "    return value.upper()\n",
    "\n",
    "udf_custom_function = udf(custom_function, StringType())\n",
    "\n",
    "df = df.withColumn('new_column', udf_custom_function(df['existing_column']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Joins in PySpark\n",
    "# Question: How would you perform a join between two DataFrames on multiple columns in PySpark?\n",
    "# Use the join() function and specify the join condition using a list of columns or expressions.\n",
    "\n",
    "df1 = None \n",
    "df2 = None \n",
    "\n",
    "df_joined = df1.join(\n",
    "    df2, \n",
    "    (df1['col1'] == df2['col1']) & (df1['col2'] == df2['col2']), \n",
    "    'inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Handling Nulls\n",
    "#Question: How would you handle missing values (nulls) in a PySpark DataFrame?\n",
    "#Answer: PySpark provides several methods to handle null values:\n",
    "    # Use fillna() to replace nulls with a specific value.\n",
    "    # Use dropna() to remove rows with null values.\n",
    "    # Use fillna() to replace nulls selectively in specific columns.\n",
    "\n",
    "df = df.fillna({'column1': 0, 'column2': 'unknown'})\n",
    "df = df.dropna(subset=['column3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DataFrame Operations\n",
    "# Question: How would you convert a DataFrame column with JSON strings into multiple columns in PySpark?\n",
    "# Answer: You can use the from_json() function along with a schema to parse the JSON strings and selectExpr() \n",
    "# to extract the fields into new columns.\n",
    "\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"field1\", StringType(), True),\n",
    "    StructField(\"field2\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = df.withColumn(\"jsonData\", from_json(df.jsonColumn, schema))\n",
    "df = df.selectExpr(\"*\", \"jsonData.*\").drop(\"jsonData\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
