{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = (`employee_id`, `name`, `age`, and `salary`)\n",
    "# Filter out employees older than 30 years.\n",
    "# Find the average salary for those employees.\n",
    "   \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(1, 'John', 28, 3000),\n",
    "             (2, 'Jane', 35, 5000),\n",
    "             (3, 'Sam', 32, 4500),\n",
    "             (4, 'Linda', 29, 3500)]\n",
    "\n",
    "df = spark.createDataFrame(data, ['employee_id', 'name', 'age', 'salary'])\n",
    "\n",
    "# Filter employees older than 30 and calculate average salary\n",
    "result = df.filter(df.age > 30).agg(avg('salary').alias('average_salary'))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GroupBy and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = ( `department`, `employee_id`, and `salary`) \n",
    "# calculate the total salary for each department.\n",
    "df.groupBy('department').sum('salary').alias('total_salary').show()\n",
    "\n",
    "\n",
    "### Group by mulitple columns \n",
    "df_grouped = df.groupBy('deparment','gender') \\\n",
    ".agg(\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    sum(\"salary\").alias(\"total_salary\")\n",
    ")\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a DataFrame with missing values in multiple columns, how would you:\n",
    "# 1. Drop rows where any column has null values?\n",
    "# 2. Replace null values in a specific column, say `age`, with the average age?\n",
    "\n",
    "from pyspark.sql.functions import col, mean\n",
    "\n",
    "# Drop rows with any null values\n",
    "df_cleaned = df.na.drop()\n",
    "\n",
    "# Replace null values in 'age' column with the average age\n",
    "avg_age = df.select(mean(col('age'))).first()[0]\n",
    "df_filled = df.na.fill({'age': avg_age})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = (`department`, `employee_id`, and `salary`)\n",
    "# write PySpark code to add a column that ranks employees in each department based on their salary.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "window_spec = Window.partitionBy('department').orderBy(df['salary'].desc())\n",
    "\n",
    "df.withColumn('rank', rank().over(window_spec)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given two DataFrames, `employees` and `departments`, \n",
    "# write PySpark code to perform an inner join on a common column `department_id`.\n",
    "employees.join(departments, on='department_id', how='inner').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. File Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write PySpark code to load a CSV file into a DataFrame, perform a transformation, and write the result back as a Parquet file.\n",
    "\n",
    "# Read CSV\n",
    "df = spark.read.csv('/path/to/file.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Perform a transformation (e.g., filter records)\n",
    "df_filtered = df.filter(df['salary'] > 4000)\n",
    "\n",
    "# Write the result to Parquet\n",
    "df_filtered.write.parquet('/path/to/output')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Optimization and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can you optimize PySpark jobs for better performance? Write code to cache a DataFrame and explain its impact.\n",
    "\n",
    "# Cache the DataFrame\n",
    "df_cached = df.cache()\n",
    "\n",
    "# Perform transformations after caching\n",
    "df_filtered = df_cached.filter(df['salary'] > 4000)\n",
    "df_filtered.show()\n",
    "\n",
    "# Explanation: Caching stores the DataFrame in memory, speeding up future actions on the same data by avoiding recomputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Question: How would you handle errors or bad records in a PySpark job when reading data from a file?\n",
    "df = spark.read.option(\"badRecordsPath\", \"/path/to/error\").csv('/path/to/file.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
