{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "focus on more advanced concepts such as \n",
    "- performance optimization, \n",
    "- complex data transformations, \n",
    "- large-scale data processing,  \n",
    "- architectural decision-making. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Optimization of PySpark Jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q) You are processing a large dataset in PySpark that involves multiple joins and aggregations. The job is slow and often times out. \n",
    "# What steps would you take to optimize the performance of the PySpark job?\n",
    "'''\n",
    "- Enable **predicate pushdown** when reading data.\n",
    "- Repartition the data intelligently based on join keys.\n",
    "- Use **broadcast joins** when one of the tables is small.\n",
    "- Cache intermediate DataFrames if they are used multiple times.\n",
    "- Avoid using `count()` or `collect()` unless absolutely necessary.\n",
    "- Set proper **shuffle partitions** based on cluster size (`spark.sql.shuffle.partitions`).\n",
    "- Use **filter** and **select** to reduce the size of DataFrames early in the process.\n",
    "'''\n",
    "# Example: Optimizing joins\n",
    "small_df = broadcast(small_df)\n",
    "large_df.join(small_df, 'join_key', 'inner').repartition('join_key').show()\n",
    "\n",
    "\n",
    "###  Performance and Optimization\n",
    "'''\n",
    "Q1) How does PySpark handle large datasets? What optimizations are available for efficient processing?\n",
    "Q2) Describe how the Catalyst optimizer works in PySpark. How does it optimize logical and physical query plans?\n",
    "\n",
    "Q3) What are some strategies for optimizing Spark SQL queries in PySpark?\n",
    "Q4) How would you handle skewed data in PySpark? What techniques can be used to minimize performance degradation?\n",
    "\n",
    "Q5) How do PySpark partitions work? How can you control the number of partitions in a DataFrame, and why is partitioning \n",
    "important for performance?\n",
    "\n",
    "Q6) What are some ways to manage memory and avoid OutOfMemory errors in PySpark jobs?\n",
    "Q7) Can you describe a situation where you needed to use custom serialization (Kryo) in PySpark? \n",
    "    Why is serialization important, and how does it affect performance?\n",
    "\n",
    "Q8) You notice that a PySpark job that used to complete in 30 minutes is now taking over 2 hours. \n",
    "    How would you troubleshoot and optimize this job?\n",
    "\n",
    "Q9) What metrics or tools would you use to diagnose performance bottlenecks in Spark?\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Handling Skewed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q) You notice that one of the partitioned columns in your data has highly skewed data, leading to slow tasks in your Spark job. \n",
    "# You are working with two datasets that need to be joined. One of the datasets, `sales_data`, is highly skewed, with 90% of the rows \n",
    "# having the same value for the `region` column.\n",
    "# How would you handle data skewness in PySpark?\n",
    "'''\n",
    "- **Salting**: Introduce artificial keys to distribute the data more evenly.\n",
    "- Use **random partitioning** to reduce the imbalance.\n",
    "- **Broadcast join** when the smaller dataset is skewed.\n",
    "- Increase the **shuffle partitions**.\n",
    "  - Write a solution to handle this skew in PySpark.\n",
    "       - Hint: Use techniques like **salting** to distribute the skewed data across multiple partitions.\n",
    "'''  \n",
    "from pyspark.sql.functions import expr, monotonically_increasing_id\n",
    "\n",
    "# Example1 of salting\n",
    "df = df.withColumn(\"salted_key\", expr(\"concat(join_key, '_', (monotonically_increasing_id() % 10))\"))\n",
    "\n",
    "\n",
    "# Example2: Add salt to distribute skewed data\n",
    "salted_sales = sales_data.withColumn(\"salted_region\", expr(\"concat(region, '_', (monotonically_increasing_id() % 10))\"))\n",
    "result = salted_sales.join(other_df, salted_sales['salted_region'] == other_df['region'], \"inner\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Data Pipeline Design**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   - **Question:** You are tasked with designing an ETL pipeline to process a daily batch of 1 TB of JSON files stored in S3 and \n",
    "# load the processed data into a Redshift table. Explain the design of your pipeline using PySpark and include optimizations for both \n",
    "# compute and storage.\n",
    "'''\n",
    "     - Use **S3 Select** to filter data while reading large JSON files.\n",
    "     - Set up **dynamic partition pruning** and **predicate pushdown**.\n",
    "     - Use **incremental loads** and watermarking to handle late data.\n",
    "     - Use **DataFrame caching** and repartitioning based on the Redshift schema.\n",
    "     - Compress the data using Parquet or ORC before loading into Redshift for space and performance efficiency.\n",
    "     - Use **copy command** or **AWS Glue** for efficient Redshift load.\n",
    "''' \n",
    "df = spark.read.json('s3://bucket/data/')\n",
    "df_filtered = df.filter(df['date'] == '2024-09-13')  # Partition pruning\n",
    "df_filtered.write.parquet('/path/to/save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Streaming and Batch Integration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain how you would integrate both batch and real-time streaming data pipelines in PySpark. \n",
    "# For example, consider you have to process clickstream data (streaming) along with daily user profile updates (batch).\n",
    "'''\n",
    " - Use **Structured Streaming** to handle real-time clickstream data.\n",
    " - Batch user profile updates are processed separately and joined with the streaming data.\n",
    " - Utilize **watermarking** to handle late data in the streaming pipeline.\n",
    " - Output both streams and batch results in the same storage format (e.g., Parquet) for consistency.\n",
    "\n",
    "'''\n",
    "\n",
    "# Example: Streaming and batch join\n",
    "streaming_df = spark.readStream.format(\"kafka\").load()\n",
    "batch_df = spark.read.parquet(\"/path/to/batch\")\n",
    "result_df = streaming_df.join(batch_df, \"user_id\", \"left_outer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Advanced Window Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are given a dataset `stock_prices` with columns `symbol`, `date`, and `price`. You are required to calculate the 30-day \n",
    "# rolling average price for each stock symbol.\n",
    "'''\n",
    "- Write a PySpark solution that computes the rolling average using **Window functions**.\n",
    "- Use **Window functions** with time-based partitioning.\n",
    "\n",
    "'''\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "window_spec = Window.partitionBy('symbol').orderBy('date').rowsBetween(-29, 0)\n",
    "result = stock_prices.withColumn('30_day_avg_price', avg('price').over(window_spec))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. **Fault Tolerance and Error Handling in PySpark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- **Question:** Describe how you would handle fault tolerance and ensure data consistency in a large-scale PySpark application \n",
    "# that processes real-time sensor data.\n",
    "'''\n",
    "- Enable **checkpointing** for long-running streaming jobs to ensure fault tolerance.\n",
    "- Handle **bad records** using `badRecordsPath` option in file-based data ingestion.\n",
    "- Implement **idempotent** writes using unique identifiers for each record.\n",
    "- Use **exactly-once semantics** with Structured Streaming and transactional sinks like Delta Lake.\n",
    "'''\n",
    "\n",
    "streaming_df.writeStream \\\n",
    "       .format(\"delta\") \\\n",
    "       .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "       .start(\"/path/to/output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. **Handling Large Data with PySpark and Partitioning Strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- **Question:** You have a dataset that exceeds 10 TB, and you need to efficiently store and process it using PySpark. \n",
    "# How would you design the storage layout and partitioning strategy to optimize read and write performance?\n",
    "'''\n",
    "- Partition the data based on high cardinality columns (e.g., `date`, `region`).\n",
    "- Use **bucketing** on frequently used join columns to reduce shuffle.\n",
    "- Store the data in **Parquet/ORC** format with snappy compression for efficient reads and writes.\n",
    "'''\n",
    "df.write \\\n",
    "       .partitionBy('date', 'region') \\\n",
    "       .bucketBy(10, 'user_id') \\\n",
    "       .format('parquet') \\\n",
    "       .save('/path/to/save')\n",
    "\n",
    "\n",
    "### 1. **Large Dataset Join Optimization**\n",
    "### You have two large datasets: `user_logs` (10 billion rows) and `user_profiles` (10 million rows). \n",
    "# The two DataFrames need to be joined on the `user_id` column. The join operation is slow, and you're running into memory issues.\n",
    "'''\n",
    "     - **Task:**\n",
    "       - Write a PySpark solution to optimize this join.\n",
    "       - Hint: Consider using **broadcast joins** or repartitioning strategies.\n",
    "'''\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Example: Use broadcast join for the smaller dataset\n",
    "result = user_logs.join(broadcast(user_profiles), \"user_id\", \"inner\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. **Custom UDFs and UDAFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Question:** Write a custom PySpark UDF that takes a string column and returns the reverse of each string. \n",
    "#How would you handle performance issues with UDFs?\n",
    "'''\n",
    "     - Use **pandas UDFs** (vectorized UDFs) for performance improvement.\n",
    "'''\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def reverse_string(s):\n",
    "    return s[::-1]\n",
    "\n",
    "df.withColumn('reversed', reverse_string(df['column'])).show()\n",
    "\n",
    "# Pandas UDF\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def reverse_string_pandas(s):\n",
    "    return s.apply(lambda x: x[::-1])\n",
    "\n",
    "df.withColumn('reversed', reverse_string_pandas(df['column'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. **Cluster Resource Management and Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- **Question:** You are working on a PySpark job in a YARN cluster that frequently runs out of memory. What configurations and tuning \n",
    "# steps would you apply to manage memory and resources effectively?\n",
    "'''\n",
    " - Increase **executor memory** and **driver memory** based on the job's requirements.\n",
    " - Tune **number of cores** and **executors** to balance parallelism and resource usage.\n",
    " - Set **spark.memory.fraction** and **spark.memory.storageFraction** to optimize memory for execution and storage.\n",
    " - Use **dynamic allocation** to optimize resource usage across different stages.\n",
    "'''\n",
    "spark = SparkSession.builder \\\n",
    "       .config(\"spark.executor.memory\", \"8g\") \\\n",
    "       .config(\"spark.executor.cores\", \"4\") \\\n",
    "       .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "       .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Processing Real-Time Streaming Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You are tasked with processing real-time clickstream data from a Kafka source. Each event contains a `user_id`, `timestamp`, and `action`. \n",
    "# You need to write a PySpark Structured Streaming job to:\n",
    "'''\n",
    "- Filter out `action = 'login'`\n",
    "- Group the data by `user_id` and calculate the total number of logins per 5-minute window.\n",
    "- Output the result to a Parquet file.\n",
    "- Implement this streaming job using PySpark.\n",
    "'''\n",
    "from pyspark.sql.functions import window, col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StreamingApp\").getOrCreate()\n",
    "\n",
    "# Read from Kafka source\n",
    "df = spark.readStream.format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"clickstream\") \\\n",
    "  .load()\n",
    "\n",
    "# Parse the value and create a DataFrame\n",
    "df_parsed = df.selectExpr(\"CAST(value AS STRING)\").alias(\"json\") \\\n",
    "  .selectExpr(\"json['user_id']\", \"json['timestamp']\", \"json['action']\")\n",
    "\n",
    "# Filter for login actions\n",
    "df_filtered = df_parsed.filter(col('action') == 'login')\n",
    "\n",
    "# Group by user and count logins within 5-minute windows\n",
    "result = df_filtered.groupBy(\n",
    "  window(col(\"timestamp\"), \"5 minutes\"), col(\"user_id\")\n",
    ").count()\n",
    "\n",
    "# Write the result to Parquet\n",
    "result.writeStream \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"path\", \"/path/to/output\") \\\n",
    "  .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Handling Large Files with Efficient Partitioning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have a large dataset (5 TB of Parquet files) stored in S3, and you are tasked with optimizing the reading process for \n",
    "# analytical queries on specific columns like `region` and `date`.\n",
    "'''\n",
    " Design an efficient PySpark read operation for this large dataset.\n",
    " Apply a partitioning strategy to optimize future reads based on `region` and `date`.\n",
    "'''\n",
    "# Read from S3 and partition the dataset by region and date\n",
    "df = spark.read.parquet(\"s3://bucket/large_dataset/\")\n",
    "     \n",
    "# Partition the dataset\n",
    "df.write.partitionBy(\"region\", \"date\").parquet(\"s3://bucket/optimized_dataset/\")\n",
    "     \n",
    "# Optimize reading by selecting specific partitions\n",
    "optimized_df = spark.read.parquet(\"s3://bucket/optimized_dataset/\") \\\n",
    "  .filter(\"region = 'US' AND date = '2024-09-13'\")\n",
    "optimized_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. **ETL Pipeline for Incremental Data** [ REDSHIFT ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to build an ETL pipeline that reads daily JSON files from an S3 bucket, processes the data, \n",
    "# and writes it to a Redshift table. The pipeline should only process new data (incremental loads).\n",
    "'''\n",
    " - Implement a PySpark solution to read and process the new files and append the result to Redshift.\n",
    " - Ensure that old data is not reprocessed.\n",
    "'''\n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETL Pipeline\").getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"s3://bucket/daily_data/2024-09-13/\")\n",
    "# Read the latest JSON files\n",
    "# Apply transformations (e.g., filter, aggregation)\n",
    "transformed_df = df.filter(df['event'] == 'purchase')\n",
    "# Write to Redshift\n",
    "transformed_df.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", \"jdbc:redshift://your-redshift-url\") \\\n",
    "  .option(\"dbtable\", \"public.purchases\") \\\n",
    "  .option(\"user\", \"username\") \\\n",
    "  .option(\"password\", \"password\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. **Custom Aggregation with UDAFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Problem:** You need to write a custom **User Defined Aggregate Function (UDAF)** in PySpark to calculate the median value for a given \n",
    "# column in a DataFrame . Write a PySpark solution using a UDAF to compute the median value of a column.\n",
    "from pyspark.sql.expressions import UserDefinedAggregateFunction\n",
    "from pyspark.sql.types import *\n",
    "class MedianUDAF(UserDefinedAggregateFunction):\n",
    "    def inputSchema(self):\n",
    "        return StructType([StructField(\"input\", DoubleType())])\n",
    "    \n",
    "    def bufferSchema(self):\n",
    "        return StructType([StructField(\"buffer\", ArrayType(DoubleType()))])\n",
    "    \n",
    "    def dataType(self):\n",
    "        return DoubleType()\n",
    "    \n",
    "    def deterministic(self):\n",
    "        return True\n",
    "    \n",
    "    def initialize(self, buffer):\n",
    "        buffer[0] = []\n",
    "    \n",
    "    def update(self, buffer, input):\n",
    "        buffer[0].append(input)\n",
    "    \n",
    "    def merge(self, buffer1, buffer2):\n",
    "        buffer1[0].extend(buffer2[0])\n",
    "    \n",
    "    def evaluate(self, buffer):\n",
    "        sorted_values = sorted(buffer[0])\n",
    "        count = len(sorted_values)\n",
    "        if count % 2 == 0:\n",
    "            return (sorted_values[count // 2 - 1] + sorted_values[count // 2]) / 2\n",
    "        else:\n",
    "            return sorted_values[count // 2]\n",
    "\n",
    "# Register and use UDAF\n",
    "spark.udf.register(\"median_udaf\", MedianUDAF())\n",
    "\n",
    "df = spark.createDataFrame([(1.0,), (2.0,), (3.0,), (4.0,), (5.0,)], [\"value\"])\n",
    "df.selectExpr(\"median_udaf(value)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. **Managing Data Lineage and Metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to ensure full data lineage and metadata tracking for your PySpark ETL pipelines.\n",
    "#Write a PySpark solution to track metadata (e.g., processing time, source file, transformation steps) for every job run and store it in a \n",
    "# separate metadata table.\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "df = spark.read.csv(\"s3://bucket/data/\")\n",
    "df = df.withColumn(\"processing_time\", current_timestamp()) \\\n",
    "            .withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "# Save data and metadata\n",
    "df.write.parquet(\"/path/to/processed_data\")\n",
    "\n",
    "metadata_df = df.select(\"processing_time\", \"source_file\").distinct()\n",
    "metadata_df.write.mode(\"append\").parquet(\"/path/to/metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fault Tolerance and Debugging\n",
    "'''\n",
    " Q) How does PySpark handle fault tolerance and recover from failures? Explain the role of lineage and RDDs.\n",
    " Q) Describe a scenario where you had to debug a PySpark job. What were the challenges, and how did you \n",
    " approach troubleshooting the issue?\n",
    "'''\n",
    "\n",
    "### Memory Management\n",
    "'''\n",
    "- A PySpark job is failing with `OutOfMemoryError`. How would you handle this issue in a production environment?\n",
    " What changes would you make to avoid this in future?\n",
    "\n",
    "'''\n",
    "\n",
    "### Data Skew\n",
    "'''\n",
    "- You are joining two large datasets, and the job is running slowly due to skewed data. How would you identify skew, \n",
    "and what steps would you take to mitigate this issue?\n",
    "\n",
    "'''\n",
    "\n",
    "### Schema Evolution\n",
    "'''\n",
    "- In your real-time PySpark pipeline, you need to process data from JSON files, but the schema changes periodically. \n",
    "How would you handle schema evolution to ensure the job doesn't break with each change?\n",
    "\n",
    "'''\n",
    "### 6. Job Scheduling and Monitoring\n",
    "'''\n",
    "- You have a series of PySpark jobs scheduled to run at specific intervals on production. One job failed due to transient network issues. \n",
    "How would you ensure the job automatically retries and succeeds?\n",
    "- What tools or strategies do you use to monitor and alert when PySpark jobs fail?\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "### 7. Data Partitioning Strategy\n",
    "'''\n",
    "- You have a large dataset in AWS S3 that you need to process daily in PySpark. How would you choose the partitioning strategy \n",
    "to minimize shuffle and optimize read performance?\n",
    "- How would you approach re-partitioning the data if it grows significantly over time?\n",
    "'''\n",
    "\n",
    "### 8. Data Consistency\n",
    "'''\n",
    "- In a real-time streaming application, you encounter issues where duplicate records are being processed. How would you ensure data \n",
    "deduplication in a PySpark streaming pipeline?\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "### 9. Cluster Resource Management\n",
    "'''\n",
    "- You are running a PySpark job on a YARN cluster, but the job is failing due to resource allocation issues (e.g., insufficient CPU or memory).\n",
    " How would you adjust resource configurations to ensure successful execution?\n",
    "- How do you manage resource allocation when multiple jobs are running simultaneously on the same cluster?\n",
    "\n",
    "'''\n",
    "\n",
    "### 10. Handling Large Joins\n",
    "'''\n",
    "- You are working with two massive datasets in PySpark, and a join operation is causing the job to fail due to memory constraints. \n",
    "How would you approach this problem?\n",
    "- How would you decide whether to use a broadcast join or repartition join in this case?\n",
    "\n",
    "'''\n",
    "\n",
    "# Question \n",
    "'''\n",
    "Q) How do you monitor the jobs \n",
    "Q) Any Problem u have come across in Prod ENV \n",
    "   * OutOfMemoryError => Increase the executor Memory\n",
    "\n",
    "Q) What are some Functional Transformation U have used in Project \n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
