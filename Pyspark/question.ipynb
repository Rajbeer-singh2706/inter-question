{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medium "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADVANCE QUESTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Real-Time Data Processing\n",
    "'''\n",
    "- How would you design a real-time data processing pipeline using PySpark? What challenges would you face in handling large amounts \n",
    "of incoming data in real time?\n",
    "- How would you ensure data consistency and handle late-arriving data in real-time streams?\n",
    "\n",
    "'''\n",
    "\n",
    "###  Performance and Optimization\n",
    "'''\n",
    "- How does PySpark handle large datasets? What optimizations are available for efficient processing?\n",
    "- Describe how the Catalyst optimizer works in PySpark. How does it optimize logical and physical query plans?\n",
    "- What are some strategies for optimizing Spark SQL queries in PySpark?\n",
    "- How would you handle skewed data in PySpark? What techniques can be used to minimize performance degradation?\n",
    "- How do PySpark partitions work? How can you control the number of partitions in a DataFrame, and why is partitioning important for performance?\n",
    "- What are some ways to manage memory and avoid OutOfMemory errors in PySpark jobs?\n",
    "- Can you describe a situation where you needed to use custom serialization (Kryo) in PySpark? Why is serialization important, \n",
    "and how does it affect performance?\n",
    "\n",
    "- You notice that a PySpark job that used to complete in 30 minutes is now taking over 2 hours. \n",
    "How would you troubleshoot and optimize this job?\n",
    "\n",
    "- What metrics or tools would you use to diagnose performance bottlenecks in Spark?\n",
    "'''\n",
    "\n",
    "### Transformations and Actions\n",
    "'''\n",
    "- Explain the difference between `map()` and `flatMap()` transformations in PySpark. In what scenarios would you prefer one over the other?\n",
    "- How would you join two large datasets in PySpark that dont fit in memory? Explain broadcast joins and other techniques.\n",
    "- Explain how checkpointing and caching differ in PySpark. When would you use each?\n",
    "'''\n",
    "\n",
    "### Fault Tolerance and Debugging\n",
    "'''\n",
    "- How does PySpark handle fault tolerance and recover from failures? Explain the role of lineage and RDDs.\n",
    "- Describe a scenario where you had to debug a PySpark job. What were the challenges, and how did you approach troubleshooting the issue?\n",
    "'''\n",
    "\n",
    "###  Windowing and Aggregations\n",
    "'''\n",
    "- How do you use Window functions in PySpark? Provide an example of a use case where Window functions are essential.\n",
    "- What are some advanced PySpark aggregation techniques for large datasets?\n",
    "\n",
    "'''\n",
    "\n",
    "### Data Structures: DataFrames and RDDs\n",
    "'''\n",
    "- What are the key differences between DataFrames and RDDs in PySpark? When would you prefer to use one over the other?\n",
    "- How does PySpark differ in handling structured vs. unstructured data?\n",
    "\n",
    "'''\n",
    "\n",
    "### Integration and Ecosystem\n",
    "'''\n",
    "- How would you integrate PySpark with other data processing systems (e.g., Hadoop, AWS S3, Redshift, etc.)?\n",
    "- Can you describe a pipeline you built using PySpark along with other tools in the big data ecosystem?\n",
    "\n",
    "'''\n",
    "\n",
    "### Advanced Topics\n",
    "'''\n",
    "- How do you handle streaming data using PySpark? Explain Spark Streaming and Structured Streaming.\n",
    "- Can you explain how to use custom user-defined functions (UDFs) in PySpark, and how to optimize their performance?\n",
    "\n",
    "'''\n",
    "\n",
    "### Memory Management\n",
    "'''\n",
    "- A PySpark job is failing with `OutOfMemoryError`. How would you handle this issue in a production environment?\n",
    " What changes would you make to avoid this in future?\n",
    "\n",
    "'''\n",
    "\n",
    "### Data Skew\n",
    "'''\n",
    "- You are joining two large datasets, and the job is running slowly due to skewed data. How would you identify skew, \n",
    "and what steps would you take to mitigate this issue?\n",
    "\n",
    "'''\n",
    "\n",
    "### Schema Evolution\n",
    "'''\n",
    "- In your real-time PySpark pipeline, you need to process data from JSON files, but the schema changes periodically. \n",
    "How would you handle schema evolution to ensure the job doesn't break with each change?\n",
    "\n",
    "'''\n",
    "\n",
    "### 6. Job Scheduling and Monitoring\n",
    "'''\n",
    "- You have a series of PySpark jobs scheduled to run at specific intervals on production. One job failed due to transient network issues. \n",
    "How would you ensure the job automatically retries and succeeds?\n",
    "- What tools or strategies do you use to monitor and alert when PySpark jobs fail?\n",
    "\n",
    "'''\n",
    "\n",
    "### 7. Data Partitioning Strategy\n",
    "'''\n",
    "- You have a large dataset in AWS S3 that you need to process daily in PySpark. How would you choose the partitioning strategy \n",
    "to minimize shuffle and optimize read performance?\n",
    "- How would you approach re-partitioning the data if it grows significantly over time?\n",
    "\n",
    "'''\n",
    "\n",
    "### 8. Data Consistency\n",
    "'''\n",
    "- In a real-time streaming application, you encounter issues where duplicate records are being processed. How would you ensure data \n",
    "deduplication in a PySpark streaming pipeline?\n",
    "\n",
    "'''\n",
    "\n",
    "### 9. Cluster Resource Management\n",
    "'''\n",
    "- You are running a PySpark job on a YARN cluster, but the job is failing due to resource allocation issues (e.g., insufficient CPU or memory).\n",
    " How would you adjust resource configurations to ensure successful execution?\n",
    "- How do you manage resource allocation when multiple jobs are running simultaneously on the same cluster?\n",
    "\n",
    "'''\n",
    "\n",
    "### 10. Handling Large Joins\n",
    "'''\n",
    "- You are working with two massive datasets in PySpark, and a join operation is causing the job to fail due to memory constraints. \n",
    "How would you approach this problem?\n",
    "- How would you decide whether to use a broadcast join or repartition join in this case?\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question \n",
    "'''\n",
    "Q) How do you monitor the jobs \n",
    "Q) Any Problem u have come across in Prod ENV \n",
    "   * OutOfMemoryError => Increase the executor Memory\n",
    "\n",
    "Q) What are some Functional Transformation U have used in Project \n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
