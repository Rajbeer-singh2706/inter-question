{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝐆𝐞𝐧𝐞𝐫𝐚𝐥 𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧𝐬\n",
    "\n",
    "1. What is PySpark, and how does it relate to Apache Spark?\n",
    "2. How do you handle big data processing using PySpark?\n",
    "3. What are the advantages of using PySpark over other big data processing frameworks?\n",
    "\n",
    "𝐏𝐲𝐒𝐩𝐚𝐫𝐤 𝐂𝐨𝐫𝐞\n",
    "\n",
    "1. Explain the concept of Resilient Distributed Datasets (RDDs) in PySpark.\n",
    "2. How do you create and manipulate DataFrames in PySpark?\n",
    "3. What is the difference between map() and filter() transformations in PySpark?\n",
    "\n",
    "𝐃𝐚𝐭𝐚𝐅𝐫𝐚𝐦𝐞𝐬 𝐚𝐧𝐝 𝐃𝐚𝐭𝐚𝐒𝐞𝐭𝐬 \n",
    "\n",
    "1. How do you optimize DataFrames for performance in PySpark?\n",
    "2. Explain the concept of DataSets in PySpark and their benefits.\n",
    "3. How do you handle missing data in DataFrames?\n",
    "\n",
    "𝐃𝐚𝐭𝐚 𝐏𝐫𝐨𝐜𝐞𝐬𝐬𝐢𝐧𝐠\n",
    "\n",
    "1. Write a PySpark code snippet to read data from HDFS.\n",
    "2. How do you perform aggregations (e.g., sum, avg) on DataFrames?\n",
    "3. Explain the concept of window functions in PySpark.\n",
    "\n",
    "𝐌𝐚𝐜𝐡𝐢𝐧𝐞 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠\n",
    "\n",
    "1. How do you build a machine learning model using PySpark MLlib?\n",
    "2. Explain the concept of feature engineering in PySpark.\n",
    "3. Write a PySpark code snippet to train a linear regression model.\n",
    "\n",
    "𝐏𝐞𝐫𝐟𝐨𝐫𝐦𝐚𝐧𝐜𝐞 𝐎𝐩𝐭𝐢𝐦𝐢𝐳𝐚𝐭𝐢𝐨𝐧\n",
    "\n",
    "1. How do you optimize PySpark code for performance?\n",
    "2. Explain the concept of caching in PySpark.\n",
    "3. How do you handle skewness in PySpark?\n",
    "\n",
    "𝐒𝐜𝐞𝐧𝐚𝐫𝐢𝐨-𝐁𝐚𝐬𝐞𝐝 𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧𝐬\n",
    "\n",
    "1. You have a large dataset with billions of records. How would you process it using PySpark?\n",
    "2. Write a PySpark code snippet to handle real-time data streaming.\n",
    "3. How would you integrate PySpark with other big data tools (e.g., Hadoop, Hive)?\n",
    "\n",
    "𝐁𝐞𝐡𝐚𝐯𝐢𝐨𝐫𝐚𝐥 𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧𝐬\n",
    "\n",
    "1. Can you describe a project where you used PySpark for big data processing?\n",
    "2. How do you stay up-to-date with new PySpark features and best practices?\n",
    "3. Can you walk me through your process for troubleshooting PySpark issues?\n",
    "\n",
    "𝐀𝐝𝐯𝐚𝐧𝐜𝐞𝐝 𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧𝐬\n",
    "\n",
    "1. Explain the concept of directed acyclic graphs (DAGs) in PySpark.\n",
    "2. How do you use PySpark's GraphFrames for graph processing?\n",
    "3. Write a PySpark code snippet to perform text processing using MLlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1. What are PySpark transformations? \n",
    "- Transformations in PySpark are lazy operations, meaning they don’t execute immediately. They only run when triggered by an \n",
    "action like 'collect()'. Common transformations include 'map()', 'filter()', and 'join()'.\n",
    "\n",
    "2. What is PySpark architecture? \n",
    "- The driver program coordinates tasks, and worker nodes (containing executors) run tasks in parallel. Executors use cores to \n",
    "process tasks, while the cluster manager allocates resources across nodes.\n",
    "\n",
    "3. What is a DAG in PySpark? \n",
    "- A DAG (Directed Acyclic Graph) is a sequence of computations. When an action like 'count()' is called, PySpark breaks the DAG \n",
    "into stages and executes the operations across the cluster.\n",
    "\n",
    "4. What are actions in PySpark? \n",
    "- Actions trigger the execution of transformations, as Spark uses lazy evaluation. Examples include 'collect()', 'count()',\n",
    " and 'show()', which return results to the driver or write data to external storage.\n",
    "\n",
    "5. Difference between narrow and wide transformations? \n",
    "- Narrow transformations process data within a single partition (e.g., 'map()'). Wide transformations require shuffling data \n",
    "across multiple partitions (e.g., 'join()'), which can be more resource-intensive.\n",
    "\n",
    "6. Difference between coalesce() and repartition()? \n",
    "- 'coalesce()' reduces the number of partitions without shuffling data, making it efficient when decreasing partitions. \n",
    "- 'repartition()' reshuffles data across partitions and can both increase or decrease partition counts.\n",
    "\n",
    "7. Difference between broadcast() and cache()? \n",
    "- 'broadcast()' sends a small dataset to all executors, making it useful for lookups or joins with large datasets. \n",
    "- 'cache()' stores a DataFrame or RDD in memory for faster reuse in iterative operations or repeated queries.\n",
    "\n",
    "8. How to create a new column with a constant value? \n",
    "- df.withColumn(\"new_col\", lit(\"constant_value\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
