{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1. What are PySpark transformations? \n",
    "- Transformations in PySpark are lazy operations, meaning they donâ€™t execute immediately. They only run when triggered by an \n",
    "action like 'collect()'. Common transformations include 'map()', 'filter()', and 'join()'.\n",
    "\n",
    "2. What is PySpark architecture? \n",
    "- The driver program coordinates tasks, and worker nodes (containing executors) run tasks in parallel. Executors use cores to \n",
    "process tasks, while the cluster manager allocates resources across nodes.\n",
    "\n",
    "3. What is a DAG in PySpark? \n",
    "- A DAG (Directed Acyclic Graph) is a sequence of computations. When an action like 'count()' is called, PySpark breaks the DAG \n",
    "into stages and executes the operations across the cluster.\n",
    "\n",
    "4. What are actions in PySpark? \n",
    "- Actions trigger the execution of transformations, as Spark uses lazy evaluation. Examples include 'collect()', 'count()',\n",
    " and 'show()', which return results to the driver or write data to external storage.\n",
    "\n",
    "5. Difference between narrow and wide transformations? \n",
    "- Narrow transformations process data within a single partition (e.g., 'map()'). Wide transformations require shuffling data \n",
    "across multiple partitions (e.g., 'join()'), which can be more resource-intensive.\n",
    "\n",
    "6. Difference between coalesce() and repartition()? \n",
    "- 'coalesce()' reduces the number of partitions without shuffling data, making it efficient when decreasing partitions. \n",
    "- 'repartition()' reshuffles data across partitions and can both increase or decrease partition counts.\n",
    "\n",
    "7. Difference between broadcast() and cache()? \n",
    "- 'broadcast()' sends a small dataset to all executors, making it useful for lookups or joins with large datasets. \n",
    "- 'cache()' stores a DataFrame or RDD in memory for faster reuse in iterative operations or repeated queries.\n",
    "\n",
    "8. How to create a new column with a constant value? \n",
    "- df.withColumn(\"new_col\", lit(\"constant_value\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
