{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ğ†ğğ§ğğ«ğšğ¥ ğğ®ğğ¬ğ­ğ¢ğ¨ğ§ğ¬\n",
    "\n",
    "1. What is PySpark, and how does it relate to Apache Spark?\n",
    "2. How do you handle big data processing using PySpark?\n",
    "3. What are the advantages of using PySpark over other big data processing frameworks?\n",
    "\n",
    "ğğ²ğ’ğ©ğšğ«ğ¤ ğ‚ğ¨ğ«ğ\n",
    "\n",
    "1. Explain the concept of Resilient Distributed Datasets (RDDs) in PySpark.\n",
    "2. How do you create and manipulate DataFrames in PySpark?\n",
    "3. What is the difference between map() and filter() transformations in PySpark?\n",
    "\n",
    "ğƒğšğ­ğšğ…ğ«ğšğ¦ğğ¬ ğšğ§ğ ğƒğšğ­ğšğ’ğğ­ğ¬ \n",
    "\n",
    "1. How do you optimize DataFrames for performance in PySpark?\n",
    "2. Explain the concept of DataSets in PySpark and their benefits.\n",
    "3. How do you handle missing data in DataFrames?\n",
    "\n",
    "ğƒğšğ­ğš ğğ«ğ¨ğœğğ¬ğ¬ğ¢ğ§ğ \n",
    "\n",
    "1. Write a PySpark code snippet to read data from HDFS.\n",
    "2. How do you perform aggregations (e.g., sum, avg) on DataFrames?\n",
    "3. Explain the concept of window functions in PySpark.\n",
    "\n",
    "ğŒğšğœğ¡ğ¢ğ§ğ ğ‹ğğšğ«ğ§ğ¢ğ§ğ \n",
    "\n",
    "1. How do you build a machine learning model using PySpark MLlib?\n",
    "2. Explain the concept of feature engineering in PySpark.\n",
    "3. Write a PySpark code snippet to train a linear regression model.\n",
    "\n",
    "ğğğ«ğŸğ¨ğ«ğ¦ğšğ§ğœğ ğğ©ğ­ğ¢ğ¦ğ¢ğ³ğšğ­ğ¢ğ¨ğ§\n",
    "\n",
    "1. How do you optimize PySpark code for performance?\n",
    "2. Explain the concept of caching in PySpark.\n",
    "3. How do you handle skewness in PySpark?\n",
    "\n",
    "ğ’ğœğğ§ğšğ«ğ¢ğ¨-ğğšğ¬ğğ ğğ®ğğ¬ğ­ğ¢ğ¨ğ§ğ¬\n",
    "\n",
    "1. You have a large dataset with billions of records. How would you process it using PySpark?\n",
    "2. Write a PySpark code snippet to handle real-time data streaming.\n",
    "3. How would you integrate PySpark with other big data tools (e.g., Hadoop, Hive)?\n",
    "\n",
    "ğğğ¡ğšğ¯ğ¢ğ¨ğ«ğšğ¥ ğğ®ğğ¬ğ­ğ¢ğ¨ğ§ğ¬\n",
    "\n",
    "1. Can you describe a project where you used PySpark for big data processing?\n",
    "2. How do you stay up-to-date with new PySpark features and best practices?\n",
    "3. Can you walk me through your process for troubleshooting PySpark issues?\n",
    "\n",
    "ğ€ğğ¯ğšğ§ğœğğ ğğ®ğğ¬ğ­ğ¢ğ¨ğ§ğ¬\n",
    "\n",
    "1. Explain the concept of directed acyclic graphs (DAGs) in PySpark.\n",
    "2. How do you use PySpark's GraphFrames for graph processing?\n",
    "3. Write a PySpark code snippet to perform text processing using MLlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1. What are PySpark transformations? \n",
    "- Transformations in PySpark are lazy operations, meaning they donâ€™t execute immediately. They only run when triggered by an \n",
    "action like 'collect()'. Common transformations include 'map()', 'filter()', and 'join()'.\n",
    "\n",
    "2. What is PySpark architecture? \n",
    "- The driver program coordinates tasks, and worker nodes (containing executors) run tasks in parallel. Executors use cores to \n",
    "process tasks, while the cluster manager allocates resources across nodes.\n",
    "\n",
    "3. What is a DAG in PySpark? \n",
    "- A DAG (Directed Acyclic Graph) is a sequence of computations. When an action like 'count()' is called, PySpark breaks the DAG \n",
    "into stages and executes the operations across the cluster.\n",
    "\n",
    "4. What are actions in PySpark? \n",
    "- Actions trigger the execution of transformations, as Spark uses lazy evaluation. Examples include 'collect()', 'count()',\n",
    " and 'show()', which return results to the driver or write data to external storage.\n",
    "\n",
    "5. Difference between narrow and wide transformations? \n",
    "- Narrow transformations process data within a single partition (e.g., 'map()'). Wide transformations require shuffling data \n",
    "across multiple partitions (e.g., 'join()'), which can be more resource-intensive.\n",
    "\n",
    "6. Difference between coalesce() and repartition()? \n",
    "- 'coalesce()' reduces the number of partitions without shuffling data, making it efficient when decreasing partitions. \n",
    "- 'repartition()' reshuffles data across partitions and can both increase or decrease partition counts.\n",
    "\n",
    "7. Difference between broadcast() and cache()? \n",
    "- 'broadcast()' sends a small dataset to all executors, making it useful for lookups or joins with large datasets. \n",
    "- 'cache()' stores a DataFrame or RDD in memory for faster reuse in iterative operations or repeated queries.\n",
    "\n",
    "8. How to create a new column with a constant value? \n",
    "- df.withColumn(\"new_col\", lit(\"constant_value\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
