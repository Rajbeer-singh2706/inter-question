{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DataFrame Operations\n",
    "# Question: How would you convert a DataFrame column with JSON strings into multiple columns in PySpark?\n",
    "# Answer: You can use the from_json() function along with a schema to parse the JSON strings and selectExpr() \n",
    "# to extract the fields into new columns.\n",
    "\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"field1\", StringType(), True),\n",
    "    StructField(\"field2\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = df.withColumn(\"jsonData\", from_json(df.jsonColumn, schema))\n",
    "df = df.selectExpr(\"*\", \"jsonData.*\").drop(\"jsonData\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Handling Nulls\n",
    "#Question: How would you handle missing values (nulls) in a PySpark DataFrame?\n",
    "#Answer: PySpark provides several methods to handle null values:\n",
    "    # Use fillna() to replace nulls with a specific value.\n",
    "    # Use dropna() to remove rows with null values.\n",
    "    # Use fillna() to replace nulls selectively in specific columns.\n",
    "\n",
    "df = df.fillna({'column1': 0, 'column2': 'unknown'})\n",
    "df = df.dropna(subset=['column3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Joins in PySpark\n",
    "# Question: How would you perform a join between two DataFrames on multiple columns in PySpark?\n",
    "# Use the join() function and specify the join condition using a list of columns or expressions.\n",
    "\n",
    "df1 = None \n",
    "df2 = None \n",
    "\n",
    "df_joined = df1.join(\n",
    "    df2, \n",
    "    (df1['col1'] == df2['col1']) & (df1['col2'] == df2['col2']), \n",
    "    'inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. UDFs (User Defined Functions)\n",
    "# Question: How would you create and use a UDF in PySpark to apply a custom function on a DataFrame column?\n",
    "# Answer: First, define the UDF using the udf decorator or function, and then apply it to the DataFrame column.\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def custom_function(value):\n",
    "    return value.upper()\n",
    "\n",
    "udf_custom_function = udf(custom_function, StringType())\n",
    "\n",
    "df = df.withColumn('new_column', udf_custom_function(df['existing_column']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Window Functions\n",
    "# Question: Can you explain how to use window functions in PySpark to calculate a moving average?\n",
    "# You can use the window function to define the partitioning and ordering, and then apply aggregation functions like avg() over this window.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "window_spec = Window.partitionBy(\"partition_col\").orderBy(\"order_col\").rowsBetween(-2, 0)\n",
    "df = df.withColumn(\"moving_avg\", avg(\"value_col\").over(window_spec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Optimization Techniques\n",
    "'''\n",
    "Question: What are some best practices for optimizing PySpark code?\n",
    " - Use select() to project only necessary columns.\n",
    " - Cache DataFrames when they are reused multiple times using cache() or persist().\n",
    "Avoid using collect() on large datasets.\n",
    "Use broadcast() for small DataFrames when joining with a large DataFrame.\n",
    "Consider using partitioning and bucketing for large datasets\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
