{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. **DataFrame Operations**\n",
    "# - **Question:** Given a DataFrame with columns `employee_id`, `name`, `age`, and `salary`, write a PySpark code to:\n",
    "#   - Filter out employees older than 30 years.\n",
    "#    - Find the average salary for those employees.\n",
    "   \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(1, 'John', 28, 3000),\n",
    "             (2, 'Jane', 35, 5000),\n",
    "             (3, 'Sam', 32, 4500),\n",
    "             (4, 'Linda', 29, 3500)]\n",
    "\n",
    "df = spark.createDataFrame(data, ['employee_id', 'name', 'age', 'salary'])\n",
    "\n",
    "# Filter employees older than 30 and calculate average salary\n",
    "result = df.filter(df.age > 30).agg(avg('salary').alias('average_salary'))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. **GroupBy and Aggregation**\n",
    "#- **Question:** Given a DataFrame with columns `department`, `employee_id`, and `salary`, write a PySpark query to calculate the total \n",
    "# salary for each department.\n",
    "df.groupBy('department').sum('salary').alias('total_salary').show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. **Handling Missing Data**\n",
    "   - **Question:** Given a DataFrame with missing values in multiple columns, how would you:\n",
    "     - Drop rows where any column has null values?\n",
    "     - Replace null values in a specific column, say `age`, with the average age?\n",
    "   - **Expected Solution:**\n",
    "     ```python\n",
    "     from pyspark.sql.functions import col, mean\n",
    "\n",
    "     # Drop rows with any null values\n",
    "     df_cleaned = df.na.drop()\n",
    "\n",
    "     # Replace null values in 'age' column with the average age\n",
    "     avg_age = df.select(mean(col('age'))).first()[0]\n",
    "     df_filled = df.na.fill({'age': avg_age})\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. **Window Functions**\n",
    "   - **Question:** Given a DataFrame containing `department`, `employee_id`, and `salary`, write PySpark code to add a column that ranks employees in each department based on their salary.\n",
    "   - **Expected Solution:**\n",
    "     ```python\n",
    "     from pyspark.sql.window import Window\n",
    "     from pyspark.sql.functions import rank\n",
    "\n",
    "     window_spec = Window.partitionBy('department').orderBy(df['salary'].desc())\n",
    "\n",
    "     df.withColumn('rank', rank().over(window_spec)).show()\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. **Joins**\n",
    "   - **Question:** Given two DataFrames, `employees` and `departments`, write PySpark code to perform an inner join on a common column `department_id`.\n",
    "   - **Expected Solution:**\n",
    "     ```python\n",
    "     employees.join(departments, on='department_id', how='inner').show()\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 6. **File Handling**\n",
    "   - **Question:** Write PySpark code to load a CSV file into a DataFrame, perform a transformation, and write the result back as a Parquet file.\n",
    "   - **Expected Solution:**\n",
    "     ```python\n",
    "     # Read CSV\n",
    "     df = spark.read.csv('/path/to/file.csv', header=True, inferSchema=True)\n",
    "\n",
    "     # Perform a transformation (e.g., filter records)\n",
    "     df_filtered = df.filter(df['salary'] > 4000)\n",
    "\n",
    "     # Write the result to Parquet\n",
    "     df_filtered.write.parquet('/path/to/output')\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. **Optimization and Performance**\n",
    "   - **Question:** How can you optimize PySpark jobs for better performance? Write code to cache a DataFrame and explain its impact.\n",
    "   - **Expected Solution:**\n",
    "     ```python\n",
    "     # Cache the DataFrame\n",
    "     df_cached = df.cache()\n",
    "\n",
    "     # Perform transformations after caching\n",
    "     df_filtered = df_cached.filter(df['salary'] > 4000)\n",
    "     df_filtered.show()\n",
    "\n",
    "     # Explanation: Caching stores the DataFrame in memory, speeding up future actions on the same data by avoiding recomputation.\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. **Error Handling**\n",
    "   - **Question:** How would you handle errors or bad records in a PySpark job when reading data from a file?\n",
    "   - **Expected Solution:**\n",
    "     ```python\n",
    "     df = spark.read.option(\"badRecordsPath\", \"/path/to/error\").csv('/path/to/file.csv')\n",
    "     ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
