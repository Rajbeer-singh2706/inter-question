{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. **How do you perform an efficient join operation between two large DataFrames in PySpark?**\n",
    "'''\n",
    "     - Using broadcast joins when one DataFrame is small enough to fit into memory.\n",
    "     - Ensuring both DataFrames are properly partitioned to avoid data shuffling.\n",
    "     - Using bucketing if the join keys are repetitive to avoid skew and optimize performance.\n",
    "     - Applying `salting` to handle skewed data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### COALESCE() AND REPARTITION()\n",
    "### 2. **Explain the difference between `coalesce()` and `repartition()` in PySpark.**\n",
    "'''\n",
    "  - **`coalesce()`**: Reduces the number of partitions in an existing RDD or DataFrame without a full shuffle. \n",
    "  It is more efficient when decreasing partitions.\n",
    "  - **`repartition()`**: Can increase or decrease the number of partitions and always triggers a full shuffle, \n",
    "  redistributing data across all nodes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEBUGGING \n",
    "### 3. **How do you debug and troubleshoot performance issues in a PySpark job?**\n",
    "'''\n",
    "     - Using the Spark UI to inspect the DAG, execution plan, and stages to identify bottlenecks.\n",
    "     - Checking for skewed data and repartitioning or salting the data as needed.\n",
    "     - Analyzing the use of wide transformations like joins and groupBy, and optimizing them.\n",
    "     - Monitoring memory and garbage collection metrics to ensure efficient use of resources.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### CACHE() AND PERSIST() \n",
    "### 4. **What are the considerations when choosing between `cache()` and `persist()` in PySpark?**\n",
    "'''\n",
    " - `cache()`**: Stores the DataFrame or RDD in memory with a default storage level of `MEMORY_ONLY`.\n",
    " - `persist()`**: Allows you to specify the storage level (e.g., `MEMORY_AND_DISK`, `DISK_ONLY`), providing more \n",
    " flexibility based on the available memory and the nature of the operations.\n",
    " - Consider using `persist()` when the dataset is too large to fit in memory or when the dataset is needed across multiple actions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Incremental Daat Processing\n",
    "### 5. **How would you handle incremental data processing in PySpark?**\n",
    "'''\n",
    "  - Using watermarking and windowing in streaming jobs to process late-arriving data.\n",
    "  - Storing the processed data state in external storage (like HDFS or S3) and reading only the new data in subsequent runs.\n",
    "  - Using Delta Lake or Apache Hudi for ACID transactions and handling upserts.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. **What is the significance of `spark.sql.autoBroadcastJoinThreshold`, and how would you tune it?**\n",
    "'''\n",
    "  - **Answer**: The `spark.sql.autoBroadcastJoinThreshold` parameter controls the maximum size of a DataFrame (in bytes) that can be \n",
    "   broadcasted to all worker nodes when performing a join. Tuning this parameter involves:\n",
    "  - Increasing it for small to medium-sized DataFrames that are used in joins to avoid shuffling.\n",
    "  - Decreasing it if broadcasting is causing memory issues on worker nodes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCHEMA EVOLUTION \n",
    "### 7. **How do you manage schema evolution in PySpark when dealing with semi-structured data like JSON?**\n",
    "'''\n",
    "     - Using the `mergeSchema` option when reading data to automatically handle changes in schema.\n",
    "     - Implementing custom schema inference logic to account for new fields or data types.\n",
    "     - Using Delta Lake to manage schema evolution with ACID guarantees, allowing for safe alterations in the schema over time.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Optimization\n",
    "### 8. **Describe the process of optimizing a PySpark job that processes data stored in a highly nested structure.**\n",
    "### Optimizing such a job involves:\n",
    "'''\n",
    "     - Flattening the nested structure using `selectExpr`, `explode`, or other DataFrame operations to simplify processing.\n",
    "     - Applying `projection pushdown` to reduce the amount of data read from the source.\n",
    "     - Using `predicate pushdown` to filter data early in the processing pipeline.\n",
    "     - Caching intermediate results if they are reused multiple times.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data Quality\n",
    "### 9. **How do you ensure data quality and consistency in a PySpark pipeline?**\n",
    "### Ensuring data quality and consistency involves:\n",
    "\n",
    "'''\n",
    "     - Implementing validation checks at each stage of the pipeline using custom functions or libraries like Deequ.\n",
    "     - Applying schema enforcement when reading data to catch errors early.\n",
    "     - Using unit tests for your transformations with a framework like PyTest.\n",
    "     - Regularly monitoring data pipelines with metrics and alerting systems.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 10. **Explain how you would implement a custom partitioner in PySpark and in what scenarios it would be beneficial.**\n",
    "'''\n",
    " - A custom partitioner can be implemented by subclassing `Partitioner` and overriding the `getPartition()` method. This approach is \n",
    " beneficial when:\n",
    " - You need to partition data based on a specific logic that the default partitioners dont cover, like partitioning based on a \n",
    " complex key structure.\n",
    " - You want to co-locate related data on the same nodes to minimize shuffling, especially in scenarios involving joins or aggregations.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
