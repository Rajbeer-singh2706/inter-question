{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizing PySpark applications is critical to improving performance, reducing execution time, and minimizing resource usage. Below are several techniques and best practices for optimizing PySpark:\n",
    "\n",
    "### **1. DataFrame Operations Optimizations**\n",
    "   - **Use DataFrames over RDDs**: DataFrames are optimized for performance through Catalyst and Tungsten (PySpark’s query optimizer), while RDDs offer no optimization. Always prefer DataFrames over RDDs unless you have a very specific use case.\n",
    "   - **Avoid User-Defined Functions (UDFs)**: PySpark UDFs are not optimized and slow down performance. Whenever possible, use built-in PySpark functions or native SQL expressions. UDFs require serialization and deserialization, which is slow.\n",
    "   - **Columnar Operations**: Leverage PySpark’s DataFrame API, which allows you to work on columns efficiently. Built-in functions like `.select()`, `.filter()`, `.withColumn()`, etc., are optimized.\n",
    "\n",
    "   ```python\n",
    "   df.select(\"column1\", \"column2\").filter(df[\"column1\"] > 100)\n",
    "   ```\n",
    "\n",
    "### **2. Caching and Persistence**\n",
    "   - **Cache Intermediate Data**: If you're going to reuse a DataFrame multiple times, persist or cache it in memory to avoid recomputation. Use `.cache()` or `.persist()`, but ensure you unpersist (`df.unpersist()`) the data after usage to release memory.\n",
    "\n",
    "   ```python\n",
    "   df.cache()\n",
    "   ```\n",
    "\n",
    "   - **Select Correct Storage Levels**: If your data doesn’t fit into memory, you can use different storage levels (e.g., MEMORY_AND_DISK, MEMORY_ONLY, DISK_ONLY) based on your workload needs.\n",
    "\n",
    "### **3. Partitioning and Shuffling**\n",
    "   - **Partitioning Data**: Use **repartition()** to reduce or increase the number of partitions based on your data size. For large datasets, increase the number of partitions to avoid data skew and excessive shuffling. Use `.coalesce()` to reduce partitions when needed (after a heavy shuffle operation).\n",
    "   \n",
    "   ```python\n",
    "   df = df.repartition(50, \"column\")  # Repartition based on a column\n",
    "   df = df.coalesce(10)  # Reduce partitions after shuffling\n",
    "   ```\n",
    "\n",
    "   - **Avoid Unnecessary Shuffles**: Shuffling is one of the most expensive operations in Spark. Minimize operations that trigger a shuffle, such as groupBy, join, or distinct. Use **broadcast joins** (discussed below) to optimize joins.\n",
    "\n",
    "### **4. Optimize Joins**\n",
    "   - **Broadcast Joins**: If one of the datasets in your join is small enough to fit into memory, use a **broadcast join** to avoid shuffling the larger dataset. Spark will broadcast the smaller dataset to all worker nodes.\n",
    "\n",
    "   ```python\n",
    "   from pyspark.sql.functions import broadcast\n",
    "   df_join = large_df.join(broadcast(small_df), \"common_column\")\n",
    "   ```\n",
    "\n",
    "   - **Skewed Data Handling**: If one side of your join has skewed data, consider using **salting** techniques to distribute the data evenly across partitions. Salting involves artificially adding a random key to break large partitions into smaller chunks.\n",
    "\n",
    "### **5. Predicate Pushdown**\n",
    "   - **Filter Early**: Use filtering as early as possible to reduce the amount of data processed. PySpark can push down filters to the source system (e.g., Parquet, ORC) for optimized I/O.\n",
    "   \n",
    "   ```python\n",
    "   df.filter(df[\"age\"] > 30).select(\"name\", \"age\")\n",
    "   ```\n",
    "\n",
    "   - **Optimize Data Source Reads**: Ensure that your data source supports predicate pushdown (e.g., Parquet, ORC). PySpark will apply filters before reading the data, reducing the amount of data loaded into memory.\n",
    "\n",
    "### **6. Serialization and Deserialization Optimizations**\n",
    "   - **Use Efficient Serialization**: Switch to **Kryo serialization** instead of the default Java serializer. Kryo is faster and uses less memory. You need to register your classes with Kryo to take advantage of this.\n",
    "\n",
    "   ```python\n",
    "   conf = SparkConf().set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "   sc = SparkContext(conf=conf)\n",
    "   ```\n",
    "\n",
    "   - **Avoid Large Objects**: Minimize the size of objects sent between the driver and workers, especially when using actions like `.collect()`, as they can lead to high serialization costs.\n",
    "\n",
    "### **7. Use Efficient File Formats**\n",
    "   - **Parquet and ORC**: Use columnar formats like **Parquet** or **ORC** for both reading and writing data. These formats are optimized for performance and offer better compression and faster query times compared to CSV or JSON.\n",
    "   - **Compression**: Choose an efficient compression format, such as **Snappy** or **Zlib**, to reduce I/O overhead without significantly increasing CPU usage.\n",
    "\n",
    "   ```python\n",
    "   df.write.format(\"parquet\").option(\"compression\", \"snappy\").save(\"s3://your-path\")\n",
    "   ```\n",
    "\n",
    "### **8. Avoid Wide Transformations**\n",
    "   - **Narrow Transformations**: Operations like **map()**, **filter()**, and **select()** are narrow transformations, which don’t require data to move across partitions. Prioritize these wherever possible.\n",
    "   - **Wide Transformations**: Operations like **groupByKey()**, **join()**, and **reduceByKey()** are wide transformations and cause data to shuffle across the network. Minimize wide transformations and use efficient alternatives like **reduceByKey()** instead of **groupByKey()**.\n",
    "\n",
    "### **9. Memory Management**\n",
    "   - **Executor Memory Tuning**: Allocate enough memory for Spark executors by tuning the `spark.executor.memory` setting. Too little memory causes frequent garbage collection, while too much can lead to inefficiencies.\n",
    "   \n",
    "   Example:\n",
    "   ```bash\n",
    "   spark-submit --executor-memory 8G --driver-memory 4G ...\n",
    "   ```\n",
    "\n",
    "   - **Off-Heap Memory Tuning**: You can enable **off-heap memory** to store data outside of the JVM heap, reducing the pressure on the garbage collector.\n",
    "   \n",
    "   ```bash\n",
    "   spark.executor.memoryOverhead=1024\n",
    "   spark.memory.offHeap.enabled=true\n",
    "   spark.memory.offHeap.size=4g\n",
    "   ```\n",
    "\n",
    "### **10. Garbage Collection Tuning**\n",
    "   - **GC Tuning**: By adjusting the garbage collection strategy and tuning the heap size, you can reduce the time spent on GC. Use tools like **G1GC** for large heap sizes and configure GC logs for deeper analysis.\n",
    "\n",
    "   ```bash\n",
    "   --conf \"spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35\"\n",
    "   ```\n",
    "\n",
    "### **11. Avoid Collecting Data to the Driver**\n",
    "   - **Limit Data Transfer to Driver**: Avoid using `.collect()`, `.count()`, and other actions that pull large amounts of data back to the driver. This can overwhelm the driver’s memory. Use `show()` or `take()` to sample small amounts of data.\n",
    "\n",
    "   ```python\n",
    "   df.show(10)  # Displays only 10 rows instead of collecting everything\n",
    "   ```\n",
    "\n",
    "### **12. Adaptive Query Execution (AQE)**\n",
    "   - **Enable Adaptive Query Execution (AQE)**: AQE dynamically optimizes query plans based on runtime statistics, such as joining strategies and partition sizes. This can lead to significant performance improvements.\n",
    "   \n",
    "   ```bash\n",
    "   spark.sql.adaptive.enabled=true\n",
    "   ```\n",
    "\n",
    "### **13. Broadcast Variables**\n",
    "   - **Use Broadcast Variables**: For small datasets or static lookup tables, broadcast variables reduce communication overhead by sending a read-only copy of the data to all worker nodes.\n",
    "\n",
    "   ```python\n",
    "   broadcast_var = sc.broadcast(lookup_table)\n",
    "   ```\n",
    "\n",
    "### **14. Parallelism**\n",
    "   - **Increase Parallelism**: Spark automatically assigns a default level of parallelism, but for larger datasets, increasing the parallelism can improve performance. Use `spark.default.parallelism` and `spark.sql.shuffle.partitions` for SQL queries.\n",
    "   \n",
    "   Example:\n",
    "   ```bash\n",
    "   spark.default.parallelism=200\n",
    "   spark.sql.shuffle.partitions=200\n",
    "   ```\n",
    "\n",
    "### **15. Avoid Skewed Data**\n",
    "   - **Skewed Data Handling**: If you have unevenly distributed data (e.g., one key having a large number of rows), it can cause performance bottlenecks. Use **salting** techniques or custom partitioning strategies to distribute the data evenly.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Key Optimizations:\n",
    "1. Use **DataFrames** instead of RDDs and minimize UDFs.\n",
    "2. **Cache and persist** frequently reused data.\n",
    "3. Optimize **joins** using broadcast and avoid unnecessary **shuffles**.\n",
    "4. Use efficient **file formats** like Parquet/ORC and enable **predicate pushdown**.\n",
    "5. Tune **memory** and **GC** settings to balance performance.\n",
    "6. Leverage **Adaptive Query Execution (AQE)** for dynamic query optimizations.\n",
    "\n",
    "By following these optimization strategies, your PySpark jobs will run faster and more efficiently, making the most out of your resources on AWS EMR or any Spark-based platform. Would you like detailed guidance on applying any specific optimization in your current setup?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
