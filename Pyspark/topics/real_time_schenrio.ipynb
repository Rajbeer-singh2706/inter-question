{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume if you have 10 GB of data daily coming into your system , \n",
    "'''\n",
    "Q) what will be architecture design for it \n",
    "Q) I want to develop using pyspark\n",
    "Q) AWs Service like ( S3, GLUE, REDSHIFT )\n",
    "OR \n",
    "Q) AWs Service like ( S3, GLUE, SNOWFLAKE )\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if i want to design the application with Q) AWs Service like ( S3, GLUE, REDSHIFT )\n",
    "'''\n",
    "If you want to design an application using AWS services like **S3**, **Glue**, and **Redshift**, here's how you can architect it to handle **10 GB of data daily** for processing, storage, and analytics.\n",
    "\n",
    "### **Architecture Overview**\n",
    "\n",
    "1. **Data Ingestion**: Store incoming data in **Amazon S3**.\n",
    "2. **Data Processing**: Use **AWS Glue** to perform ETL (Extract, Transform, Load) jobs, written in PySpark.\n",
    "3. **Data Storage and Analytics**: Load the processed data into **Amazon Redshift** for querying and analytics.\n",
    "4. **Orchestration & Monitoring**: Use AWS Lambda, Step Functions, and CloudWatch for workflow automation and monitoring.\n",
    "\n",
    "### **Architecture Components**\n",
    "\n",
    "1. **Amazon S3 (Data Lake for Storage)**\n",
    "   - **Raw Data Storage**: All incoming raw data is stored in **S3** in a structured format. You can organize the data using folder structures like `s3://your-bucket/raw/yyyy/mm/dd/` to store the daily incoming data.\n",
    "   - **Curated Data Storage**: After processing the data through Glue, store the cleaned and transformed data back in S3. Use optimized formats like **Parquet** or **ORC** for efficient querying.\n",
    "   - **Data Versioning**: Enable versioning on S3 buckets to manage historical data and ensure backup/recovery.\n",
    "\n",
    "2. **AWS Glue (ETL with PySpark)**\n",
    "   - **Data Transformation**: Use AWS Glue to run ETL jobs written in **PySpark** to clean, transform, and prepare the data for analytics. Glue can read the raw data from S3, perform the necessary transformations, and write the output back to S3 or directly into Redshift.\n",
    "   - **Schema Inference**: Glue can infer schemas dynamically, making it easier to handle semi-structured data.\n",
    "   - **Partitioning**: Use Glue to partition the data based on date or other relevant fields to improve query performance.\n",
    "\n",
    "   Example of Glue ETL Script (PySpark):\n",
    "   ```python\n",
    "   from pyspark.context import SparkContext\n",
    "   from awsglue.context import GlueContext\n",
    "   from awsglue.transforms import *\n",
    "   from awsglue.utils import getResolvedOptions\n",
    "\n",
    "   sc = SparkContext()\n",
    "   glueContext = GlueContext(sc)\n",
    "\n",
    "   # Read raw data from S3\n",
    "   datasource = glueContext.create_dynamic_frame.from_catalog(\n",
    "       database=\"your_database\", \n",
    "       table_name=\"your_table\"\n",
    "   )\n",
    "\n",
    "   # Apply transformations\n",
    "   transformed_data = ApplyMapping.apply(\n",
    "       frame=datasource, \n",
    "       mappings=[(\"col1\", \"string\", \"col1\", \"string\"), (\"col2\", \"int\", \"col2\", \"int\")]\n",
    "   )\n",
    "\n",
    "   # Write transformed data back to S3 in Parquet format\n",
    "   glueContext.write_dynamic_frame.from_options(\n",
    "       frame=transformed_data, \n",
    "       connection_type=\"s3\", \n",
    "       connection_options={\"path\": \"s3://your-bucket/curated/\"},\n",
    "       format=\"parquet\"\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Amazon Redshift (Data Warehouse for Analytics)**\n",
    "   - **Redshift Data Loading**: Once the data is processed and stored in S3, you can load it into **Amazon Redshift** for fast querying. Use **Redshift’s COPY command** to ingest large volumes of data efficiently.\n",
    "     - Redshift can ingest Parquet, CSV, JSON, and other formats from S3 directly.\n",
    "     - You can set up **Amazon Redshift Spectrum** to query the data stored in S3 directly without moving it into Redshift tables if you want to reduce data movement.\n",
    "   - **Data Partitioning and Distribution**: Define distribution keys and sort keys to optimize query performance and speed up analytics in Redshift.\n",
    "\n",
    "   Example Redshift COPY Command:\n",
    "   ```sql\n",
    "   COPY your_table\n",
    "   FROM 's3://your-bucket/curated/'\n",
    "   IAM_ROLE 'arn:aws:iam::your-account-id:role/your-redshift-role'\n",
    "   FORMAT AS PARQUET;\n",
    "   ```\n",
    "\n",
    "   - **Amazon Redshift Spectrum** (Optional): For querying the data in S3 directly without loading it into Redshift, Spectrum allows Redshift users to perform SQL queries on data stored in S3 using external tables.\n",
    "\n",
    "4. **Orchestration & Monitoring**\n",
    "   - **AWS Lambda**: Trigger Glue jobs automatically when new data is uploaded to S3. You can create event-based triggers to initiate ETL workflows.\n",
    "   - **AWS Step Functions**: Orchestrate complex workflows where you might have multiple Glue jobs and Redshift loading steps. It allows you to sequence tasks, manage retries, and monitor progress.\n",
    "   - **Amazon CloudWatch**: Monitor the status of Glue jobs, Lambda functions, and Redshift queries. Set up alarms to notify you of any failures or performance issues.\n",
    "\n",
    "---\n",
    "\n",
    "### **High-Level Architecture Workflow**\n",
    "\n",
    "1. **Data Ingestion into S3**:\n",
    "   - Incoming raw data is stored in **Amazon S3** (Raw Bucket).\n",
    "   - The data could come from various sources, including third-party APIs, databases, and real-time systems.\n",
    "\n",
    "2. **ETL with AWS Glue**:\n",
    "   - Glue ETL jobs written in **PySpark** read from S3 (Raw Bucket).\n",
    "   - Perform data cleaning, transformation, and enrichment.\n",
    "   - Write the cleaned data back to **S3 (Curated Bucket)** or directly into **Amazon Redshift**.\n",
    "\n",
    "3. **Data Loading into Amazon Redshift**:\n",
    "   - Use **COPY commands** to load the processed data from S3 into Redshift.\n",
    "   - If you prefer not to load the data, **Redshift Spectrum** can query the S3 data directly.\n",
    "\n",
    "4. **Analytics and Reporting**:\n",
    "   - Use Redshift’s SQL-based analytics for fast querying of large datasets.\n",
    "   - Connect Redshift to reporting tools like **Amazon QuickSight** or **Tableau** for data visualization.\n",
    "\n",
    "5. **Orchestration & Monitoring**:\n",
    "   - Automate data workflows using **Lambda** for event-driven processing and **Step Functions** for orchestrating Glue jobs and Redshift loading.\n",
    "   - Use **CloudWatch** for tracking the performance and health of your data pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Architecture Diagram**\n",
    "\n",
    "1. **Ingestion**: Data is ingested into **Amazon S3** (Raw Data).\n",
    "2. **Processing**: AWS Glue processes and transforms the data, writing back to **S3** or loading into **Redshift**.\n",
    "3. **Storage & Analytics**: Redshift stores the processed data, and **Redshift Spectrum** can query the data directly from S3 for additional flexibility.\n",
    "4. **Orchestration & Monitoring**: Use Lambda, Step Functions, and CloudWatch to automate and monitor the workflow.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cost Considerations**\n",
    "\n",
    "- **Amazon S3**: Pay for storage and requests (GET, PUT). For 10 GB/day, this is low-cost.\n",
    "- **AWS Glue**: Billed based on Data Processing Units (DPUs) used per hour. You can optimize this by controlling the size and frequency of Glue jobs.\n",
    "- **Amazon Redshift**: Billed based on the size of your Redshift cluster (node type, number of nodes). Consider Redshift **RA3** instances, which decouple storage and compute, allowing you to scale storage independently.\n",
    "- **Redshift Spectrum**: Querying S3 data directly incurs per-TB scanned costs, but it reduces the need for large data transfers.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like more details on any specific aspect, such as optimizing Glue jobs, writing Redshift queries, or setting up the Redshift Spectrum?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if i want to design the application with Q) AWs Service like ( S3, GLUE, SNOWFLAKE )\n",
    "If you're designing an application using AWS services like **S3**, **Glue**, and **Snowflake**, here's how you can architect a solution for processing daily 10 GB of data:\n",
    "\n",
    "### **Architecture Overview**\n",
    "\n",
    "1. **Data Ingestion**: Store incoming raw data in **Amazon S3**.\n",
    "2. **Data Processing**: Use **AWS Glue** for ETL (Extract, Transform, Load) jobs.\n",
    "3. **Data Storage and Analytics**: Load the processed data into **Snowflake** for querying and analytics.\n",
    "4. **Orchestration & Monitoring**: Use additional AWS services (like Lambda, Step Functions, CloudWatch) for workflow management and monitoring.\n",
    "\n",
    "### **Architecture Components**\n",
    "\n",
    "1. **Amazon S3 (Data Lake for Storage)**\n",
    "   - **Raw Data Storage**: All incoming data is stored in S3 in a raw, unprocessed format. You can organize the data in S3 with a structured folder system (e.g., `s3://your-bucket/raw/yyyy/mm/dd/`) to store daily incoming data.\n",
    "   - **Curated/Processed Data Storage**: After processing the data through Glue, store the cleaned and transformed data in another S3 bucket, preferably in columnar formats like **Parquet** or **ORC** for efficient querying.\n",
    "   - **Data Versioning**: Enable versioning on the S3 buckets to track changes to files.\n",
    "\n",
    "2. **AWS Glue (ETL Jobs in PySpark)**\n",
    "   - **Data Transformation**: Use Glue to run ETL jobs in PySpark. Glue provides a managed Spark environment where you can write PySpark scripts to transform and clean the data.\n",
    "   - **Dynamic Frames**: Glue supports dynamic frames, which are more schema-flexible than traditional Spark data frames. This makes it easier to work with semi-structured data.\n",
    "   - **Integration with S3**: Glue reads data from your S3 raw bucket, processes it, and then writes it back to the S3 curated bucket.\n",
    "   - **Partitioning**: Partition your output data by relevant fields (e.g., date) to optimize future querying and reduce processing time.\n",
    "   \n",
    "   Example Glue PySpark Script:\n",
    "   ```python\n",
    "   import sys\n",
    "   from pyspark.context import SparkContext\n",
    "   from awsglue.context import GlueContext\n",
    "   from awsglue.transforms import *\n",
    "   from awsglue.utils import getResolvedOptions\n",
    "\n",
    "   sc = SparkContext()\n",
    "   glueContext = GlueContext(sc)\n",
    "\n",
    "   # Reading from S3\n",
    "   datasource = glueContext.create_dynamic_frame.from_catalog(\n",
    "       database=\"your_database\", \n",
    "       table_name=\"your_table\"\n",
    "   )\n",
    "\n",
    "   # Apply transformations (e.g., filtering, mapping)\n",
    "   transformed_data = ApplyMapping.apply(\n",
    "       frame=datasource, \n",
    "       mappings=[(\"column1\", \"string\", \"column1\", \"string\"), (\"column2\", \"int\", \"column2\", \"int\")]\n",
    "   )\n",
    "\n",
    "   # Write back to S3 in Parquet format\n",
    "   glueContext.write_dynamic_frame.from_options(\n",
    "       frame=transformed_data, \n",
    "       connection_type=\"s3\", \n",
    "       connection_options={\"path\": \"s3://your-bucket/curated/\"},\n",
    "       format=\"parquet\"\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Snowflake (Data Warehouse for Analytics)**\n",
    "   - **Loading Data from S3**: Snowflake provides a seamless integration with S3 for loading data. After the data is processed and stored in S3, you can use **Snowflake's COPY command** to ingest this data.\n",
    "     - Create an external stage in Snowflake that points to the S3 bucket.\n",
    "     - Load the Parquet data directly into Snowflake tables for fast, scalable analytics.\n",
    "   - **SQL Analytics**: Once the data is in Snowflake, you can leverage SQL queries for further analysis, business intelligence, and reporting. Snowflake’s scaling capabilities make it perfect for running complex queries on large datasets.\n",
    "\n",
    "   Example of loading data from S3 into Snowflake:\n",
    "   ```sql\n",
    "   CREATE OR REPLACE STAGE my_s3_stage\n",
    "   URL='s3://your-bucket/curated/'\n",
    "   CREDENTIALS=(AWS_KEY_ID='your-key-id' AWS_SECRET_KEY='your-secret-key');\n",
    "\n",
    "   -- Load Parquet data into Snowflake table\n",
    "   COPY INTO your_snowflake_table\n",
    "   FROM @my_s3_stage\n",
    "   FILE_FORMAT = (TYPE = 'PARQUET');\n",
    "   ```\n",
    "\n",
    "4. **Orchestration & Monitoring**\n",
    "   - **AWS Lambda**: You can use Lambda to trigger the Glue jobs when new data is uploaded to S3 (event-driven architecture).\n",
    "   - **AWS Step Functions**: For more complex workflows (e.g., sequential Glue jobs or integrating with Snowflake), Step Functions can help orchestrate multiple steps, including monitoring and retries.\n",
    "   - **Amazon CloudWatch**: Set up CloudWatch alarms and logs to monitor the performance of Glue jobs and other components. Glue job failures, long-running processes, or data pipeline delays can all be tracked here.\n",
    "\n",
    "---\n",
    "\n",
    "### **High-Level Architecture Workflow**\n",
    "\n",
    "1. **Data Ingestion into S3**:\n",
    "   - Raw data flows into **S3 (Raw Bucket)**. \n",
    "   - Data could come from various sources like APIs, databases, third-party integrations, or even real-time streams.\n",
    "\n",
    "2. **ETL with AWS Glue**:\n",
    "   - **Glue ETL Jobs** (PySpark) read from the S3 raw bucket.\n",
    "   - Perform data cleaning, transformation, and enrichment.\n",
    "   - Write the processed data back into **S3 (Curated Bucket)** in optimized formats like Parquet.\n",
    "\n",
    "3. **Data Loading into Snowflake**:\n",
    "   - Use **Snowflake's COPY command** to ingest the curated data from S3 into Snowflake.\n",
    "   - Load the data into Snowflake tables for further analysis.\n",
    "\n",
    "4. **Analytics and Visualization**:\n",
    "   - Use **Snowflake** for fast, scalable SQL analytics.\n",
    "   - Connect Snowflake to tools like **Tableau** or **Amazon QuickSight** for data visualization and reporting.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Architecture Diagram**\n",
    "\n",
    "1. **Ingestion**: Data enters **Amazon S3** in its raw form.\n",
    "2. **ETL**: AWS Glue processes the data using PySpark, storing results back into **S3**.\n",
    "3. **Storage & Analytics**: Snowflake loads processed data from S3 and allows for fast, SQL-based analytics.\n",
    "4. **Orchestration & Monitoring**: Use Lambda for triggers, Step Functions for orchestration, and CloudWatch for monitoring and alerting.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cost Considerations**\n",
    "\n",
    "- **S3**: Pay for storage and requests (GET, PUT). For 10 GB/day, the costs are relatively low.\n",
    "- **Glue**: Costs are based on Data Processing Units (DPUs) used per hour. You can use auto-scaling to optimize Glue costs.\n",
    "- **Snowflake**: Pricing is based on compute usage (virtual warehouses) and storage. Snowflake can scale elastically, so you only pay for the compute resources you use when running queries.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to dive deeper into any specific part, like writing Glue jobs or optimizing Snowflake queries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume if i m runing the solution in AWS EMR, how the cluster configuration look like meanshow many master & worker node configuration\n",
    "'''\n",
    "When configuring an **AWS EMR cluster** to process **10 GB of data daily** using **PySpark**, the cluster's configuration, including the number and type of master and worker nodes, depends on several factors such as the complexity of your transformations, data structure, and processing time requirements.\n",
    "\n",
    "Here’s a general guideline for cluster configuration:\n",
    "\n",
    "### **Cluster Configuration Overview**\n",
    "1. **Master Node**: \n",
    "   - The master node handles the coordination of the cluster, running the resource manager and job tracker (e.g., YARN Resource Manager, Spark Master). For most cases, a **single master node** is sufficient.\n",
    "   - Recommended Instance Type: **m5.xlarge** (4 vCPUs, 16 GiB memory) or **m5.2xlarge** (8 vCPUs, 32 GiB memory) for higher reliability.\n",
    "\n",
    "2. **Core Nodes (Worker Nodes)**:\n",
    "   - Core nodes run the actual Spark executors and manage HDFS if you're using it.\n",
    "   - A good rule of thumb for starting with 10 GB of data is to have around **2-4 core nodes**, depending on the processing requirements and the time you want to spend processing the data.\n",
    "   - For instance types, **r5.xlarge** (4 vCPUs, 32 GiB memory) is a balanced choice for compute and memory.\n",
    "   - If you want more parallelism, consider using **r5.2xlarge** (8 vCPUs, 64 GiB memory) to speed up Spark jobs.\n",
    "\n",
    "3. **Task Nodes (Optional)**:\n",
    "   - Task nodes are additional worker nodes that can be added to speed up processing but do not run HDFS daemons. They’re useful for additional compute but not for storage.\n",
    "   - If you need additional capacity during peak processing, you can use task nodes as spot instances to reduce costs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sample Cluster Configuration** (For 10 GB Data)\n",
    "\n",
    "1. **Master Node**: \n",
    "   - Instance Type: **m5.xlarge**\n",
    "   - Quantity: **1**\n",
    "\n",
    "2. **Core Nodes**:\n",
    "   - Instance Type: **r5.xlarge**\n",
    "   - Quantity: **3** (start with 3, adjust based on workload)\n",
    "\n",
    "3. **Task Nodes** (Optional):\n",
    "   - Instance Type: **r5.xlarge** or **r5.2xlarge** (using Spot Instances for cost efficiency)\n",
    "   - Quantity: **2-4** (optional, to increase parallelism for compute-heavy workloads)\n",
    "\n",
    "---\n",
    "\n",
    "### **Additional Considerations**\n",
    "\n",
    "- **Auto-scaling**: \n",
    "   - Enable auto-scaling for your EMR cluster, which adjusts the number of core and task nodes based on the workload. This helps in handling data surges while keeping costs down during idle periods.\n",
    "\n",
    "- **Spot Instances for Task Nodes**:\n",
    "   - To reduce costs, you can add task nodes using spot instances. These are cheaper than on-demand nodes but come with the risk of being terminated by AWS. For non-critical tasks, spot instances can be a great cost-saving option.\n",
    "\n",
    "- **Disk Space**:\n",
    "   - For processing **10 GB daily**, each worker node should have enough disk space for intermediate storage. Depending on your use case (e.g., if the data is compressed or involves heavy shuffling), you might need **EBS volumes** attached to the instances. Start with **100-200 GB of EBS** per worker node, and adjust based on performance needs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Configuration Example**\n",
    "\n",
    "| Node Type  | Instance Type | vCPUs | Memory (GiB) | EBS Volumes | Quantity |\n",
    "|------------|---------------|-------|--------------|-------------|----------|\n",
    "| Master     | m5.xlarge     | 4     | 16           | 100-200 GB  | 1        |\n",
    "| Core       | r5.xlarge     | 4     | 32           | 100-200 GB  | 3        |\n",
    "| Task (Spot)| r5.2xlarge    | 8     | 64           | 100-200 GB  | 2-4      |\n",
    "\n",
    "This configuration should give you a good balance of cost and performance for processing your daily data with PySpark on EMR.\n",
    "\n",
    "Would you like further details on any specific aspect, such as auto-scaling policies or optimization?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If you have 10 GB of data coming into your system daily and plan to use PySpark to process it, here's a high-level AWS architecture that can help you handle this scale of data efficiently. \n",
    "\n",
    "### **Architecture Design**\n",
    "\n",
    "1. **Data Ingestion Layer**\n",
    "   - **Amazon S3**: Store the raw incoming data in Amazon S3 buckets. S3 is highly scalable and cost-efficient, making it a perfect landing zone for raw data.\n",
    "     - If the data arrives from various sources, such as APIs, databases, or third-party systems, you can configure ingestion methods like AWS Lambda or AWS Glue to load it into S3.\n",
    "     - You can use a structured folder layout based on date (`/data/YYYY/MM/DD/`) to make it easier to query and partition.\n",
    "\n",
    "2. **ETL (Extract, Transform, Load) Layer**\n",
    "   - **AWS Glue**: Use AWS Glue to create ETL jobs that can read, transform, and load the data into your target format (e.g., Parquet, ORC).\n",
    "     - **PySpark**: AWS Glue is natively built on top of Apache Spark, so you can write and run your PySpark scripts here. This can handle transformations, data cleaning, and enriching your data.\n",
    "     - Glue jobs can be triggered by S3 events, or you can schedule them based on your data arrival time.\n",
    "     - **Partitioning and Compaction**: As your data grows, you can partition the data (e.g., by date) and compact small files to optimize query performance.\n",
    "\n",
    "3. **Data Processing Layer**\n",
    "   - **Amazon EMR (Elastic MapReduce)**: If your ETL or data transformation logic is more complex and needs to scale out with high concurrency, you can leverage Amazon EMR clusters to run PySpark applications.\n",
    "     - EMR gives you more flexibility and control over your Spark environment (e.g., tuning the cluster size, configuring custom libraries).\n",
    "     - EMR also integrates with S3 for both input and output data and can scale automatically based on the size of your dataset.\n",
    "\n",
    "4. **Data Storage Layer**\n",
    "   - **Amazon S3 (Curated Layer)**: After the data is cleaned and transformed, store the output in another S3 bucket for downstream processing or querying.\n",
    "     - Store data in a columnar format (e.g., **Parquet** or **ORC**) to improve query performance.\n",
    "     - Consider using **S3 Object Lock** for versioning and immutability.\n",
    "  \n",
    "   - **Amazon Redshift** (Optional): If you need fast, SQL-based analytics on top of the processed data, you can load it into Redshift or use **Redshift Spectrum** to query the data directly from S3 without moving it.\n",
    "\n",
    "5. **Data Analytics Layer**\n",
    "   - **Amazon Athena**: You can use Athena to run SQL queries directly on top of the S3 data (e.g., Parquet files). This serverless service lets you perform interactive queries without provisioning infrastructure.\n",
    "     - Since you're processing data using PySpark, Athena will be useful for lightweight querying and validation without needing a fully managed database.\n",
    "  \n",
    "   - **Amazon QuickSight**: For data visualization, you can use QuickSight to build dashboards on top of your processed data, either through Athena, Redshift, or directly from S3.\n",
    "\n",
    "6. **Orchestration & Monitoring**\n",
    "   - **AWS Lambda**: For serverless orchestration of data pipelines, Lambda can trigger Glue jobs, monitor data arrivals in S3, or process smaller tasks like validation.\n",
    "   \n",
    "   - **AWS Step Functions**: To manage complex workflows and orchestrate multiple Glue/EMR jobs, Step Functions can help sequence tasks, handle retries, and monitor job progress.\n",
    "\n",
    "   - **Amazon CloudWatch**: Use CloudWatch to monitor the health of your Spark jobs, Glue pipelines, and the data ingestion layer. Set alarms to track job performance and troubleshoot any failures.\n",
    "\n",
    "### **High-Level Flow**\n",
    "\n",
    "1. **Data Ingestion**\n",
    "   - Data flows into **S3 (Raw Bucket)**.\n",
    "   - Ingestion methods like Lambda or Glue may load data automatically.\n",
    "\n",
    "2. **Data Processing**\n",
    "   - PySpark scripts run on **AWS Glue** or **EMR** to clean and transform the data.\n",
    "   - Transformed data is stored in **S3 (Curated Bucket)** in columnar formats like Parquet.\n",
    "\n",
    "3. **Data Querying**\n",
    "   - **Athena** or **Redshift** queries the processed data for analysis.\n",
    "   - **QuickSight** visualizes the analytics.\n",
    "\n",
    "4. **Orchestration & Monitoring**\n",
    "   - Glue jobs or Spark jobs orchestrated using **Step Functions**.\n",
    "   - **CloudWatch** monitors the entire pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### **PySpark Development in AWS Glue**\n",
    "\n",
    "- You can write and run your PySpark code directly in AWS Glue jobs. For example:\n",
    "  \n",
    "```python\n",
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "\n",
    "# Reading from S3\n",
    "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database = \"your_database\", \n",
    "    table_name = \"your_table\"\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "transformed_data = ApplyMapping.apply(\n",
    "    frame = datasource, \n",
    "    mappings = [\n",
    "        (\"column1\", \"string\", \"column1\", \"string\"),\n",
    "        (\"column2\", \"int\", \"column2\", \"int\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Write back to S3 in Parquet format\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    frame = transformed_data, \n",
    "    connection_type = \"s3\", \n",
    "    connection_options = {\"path\": \"s3://your-bucket/processed/\"},\n",
    "    format = \"parquet\"\n",
    ")\n",
    "```\n",
    "\n",
    "You can also extend this logic to include partitioning, filtering, and more complex transformations.\n",
    "\n",
    "Would you like to dive deeper into any specific part of this architecture?\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
