{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Broadcast Join\n",
    "Description: A broadcast join is a performance optimization where the smaller DataFrame is broadcasted to all the worker nodes, allowing for a much faster join when working with large datasets. Spark automatically uses broadcast join if it detects that the DataFrame size is below a threshold, but you can force it.\n",
    "Use Case: When one DataFrame is much smaller than the other, and you want to avoid shuffling the larger DataFrame.\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df1.join(broadcast(df2), df1['id'] == df2['id']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizing Joins in PySpark\n",
    "'''\n",
    " * Partitioning: Ensure both DataFrames are appropriately partitioned to avoid shuffling large amounts of data.\n",
    " * Broadcast Join: Use it for joining large datasets with small ones.\n",
    " * Skew Handling: If one of your DataFrames has skewed data (i.e., some keys appear much more often than others), this can lead to \n",
    "performance bottlenecks. Use salting techniques to handle skewed joins.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In **PySpark**, joins are performed internally using different strategies, based on the size of the datasets being joined and the type of \n",
    "#join operation. Heres a breakdown of how joins are processed and the underlying mechanics:\n",
    "\n",
    "### 1. **Shuffling and Data Exchange**\n",
    "'''\n",
    "   When PySpark performs a join between two large datasets, it needs to bring matching keys together. This often requires **shuffling**, \n",
    "   which is the process of redistributing data across the cluster. The rows with the same key from different partitions and nodes are \n",
    "   brought to the same node for the join to be performed.\n",
    "\n",
    "   - **Shuffling** involves:\n",
    "     - Partitioning the data by key.\n",
    "     - Moving data across the network between different nodes.\n",
    "     - Sorting the data to prepare for the join.\n",
    "\n",
    "   Shuffling can be very expensive, especially with large datasets, because it involves reading and writing intermediate data to disk \n",
    "   and transferring it across the network.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. **Join Algorithms**\n",
    "'''\n",
    "   PySpark uses different algorithms to execute the join, depending on the characteristics of the datasets and the type of join:\n",
    "'''\n",
    "#### 2.1. **Sort-Merge Join**\n",
    "'''\n",
    "   - **Description**: This is the default algorithm for large joins in PySpark. It is used when both datasets are too large to fit in \n",
    "   memory and need to be shuffled and sorted before joining.\n",
    "   - **Steps**:\n",
    "     1. **Shuffle** the data from both datasets so that rows with the same key end up on the same partition.\n",
    "     2. **Sort** the data in each partition by the join key.\n",
    "     3. **Merge** the sorted partitions by comparing keys and producing the joined output.\n",
    "\n",
    "   - **Best for**: Large datasets that are already partitioned or sorted by key.\n",
    "\n",
    "   - **Example Scenario**:\n",
    "     If two large DataFrames are joined on a column `id`, the records are shuffled and sorted based on `id` in each partition, and \n",
    "     then a merge process is executed to match records.\n",
    "\n",
    "   - **Performance Impact**: While sort-merge join is efficient for large datasets, it incurs a high cost due to the shuffle and sort phases.\n",
    "'''\n",
    "\n",
    "#### 2.2. **Broadcast Join**\n",
    "'''\n",
    "   - **Description**: A **broadcast join** occurs when one dataset is much smaller than the other. The smaller dataset is broadcasted to all nodes, allowing the larger dataset to be joined without shuffling. This prevents the overhead of a full shuffle for the larger dataset.\n",
    "   - **Steps**:\n",
    "     1. PySpark copies the smaller dataset (broadcast) to every node in the cluster.\n",
    "     2. The larger dataset is scanned, and for each row, the corresponding row in the smaller (broadcasted) dataset is fetched and joined.\n",
    "\n",
    "   - **Best for**: Joins where one of the DataFrames is small enough to fit into memory of all nodes.\n",
    "\n",
    "   - **Example Scenario**:\n",
    "     If you have a large sales dataset and a small lookup table (e.g., mapping country codes to country names), the lookup table can be broadcasted to avoid shuffling the large dataset.\n",
    "\n",
    "   - **Performance Impact**: **Broadcast joins** are very fast for small datasets but should only be used when the broadcasted DataFrame fits in memory.\n",
    "\n",
    "   - **Note**: PySpark automatically performs broadcast joins when it detects that the smaller dataset is below a certain size threshold, but you can manually force it using the `broadcast()` function.\n",
    "'''\n",
    "\n",
    "#### 2.3. **Shuffle Hash Join**\n",
    "'''\n",
    "   - **Description**: In this join, the smaller dataset is used to build a hash map, and the larger dataset is scanned and matched against the hash map. It requires a shuffle of the larger dataset but avoids a full sort, which can be more efficient in some cases.\n",
    "   - **Steps**:\n",
    "     1. **Shuffle** the data based on the join key to ensure that rows with matching keys are in the same partition.\n",
    "     2. Build a **hash map** from the smaller dataset.\n",
    "     3. For each row in the larger dataset, check the hash map for a matching key.\n",
    "  \n",
    "   - **Best for**: Situations where one dataset is much smaller but not small enough for broadcast.\n",
    "\n",
    "   - **Performance Impact**: Hash joins are generally faster than sort-merge joins because they avoid sorting, but they still incur a shuffle.\n",
    "'''\n",
    "\n",
    "#### 2.4. **Broadcast Hash Join**\n",
    "'''\n",
    "   - **Description**: A variant of the hash join where the smaller dataset is broadcasted to all nodes, and then each node builds a hash map of the broadcasted data.\n",
    "   - **Steps**:\n",
    "     1. **Broadcast** the smaller dataset to every node.\n",
    "     2. Build a **hash map** from the smaller dataset on each node.\n",
    "     3. For each row in the larger dataset, check the hash map for a matching key.\n",
    "\n",
    "   - **Best for**: Smaller datasets where shuffling can be avoided altogether.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 3. **Join Execution Plan**\n",
    "'''\n",
    "PySpark provides insight into the execution of a join using the `explain()` method, which shows the **logical** and **physical plan** of \n",
    "the join operation. This is crucial for understanding how PySpark will execute the join and whether any optimizations (like broadcast join) \n",
    "are being applied.\n",
    "\n",
    "   Example:\n",
    "   ```python\n",
    "   df1.join(df2, 'id').explain()\n",
    "   ```\n",
    "   Output (example):\n",
    "   ```\n",
    "   == Physical Plan ==\n",
    "   *(5) SortMergeJoin [id#4L], [id#9L], Inner\n",
    "   :- *(2) Sort [id#4L ASC NULLS FIRST], false, 0\n",
    "   :  +- Exchange hashpartitioning(id#4L, 200), true, [id=#37]\n",
    "   +- *(4) Sort [id#9L ASC NULLS FIRST], false, 0\n",
    "      +- Exchange hashpartitioning(id#9L, 200), true, [id=#38]\n",
    "   ```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. **Optimizing Joins**\n",
    "#PySpark offers several ways to optimize joins:\n",
    "\n",
    "# 1.**Broadcasting Small Tables**: For joins where one table is significantly smaller, broadcasting can reduce shuffle costs. \n",
    "# Use `broadcast()` to force broadcasting if needed.\n",
    "from pyspark.sql.functions import broadcast\n",
    "df1.join(broadcast(df2), 'id').show()\n",
    "\n",
    "\n",
    "#2. **Partitioning**: If your data is already partitioned on the join key, PySpark will avoid an expensive shuffle. \n",
    "# Ensure that both datasets are partitioned similarly.\n",
    "df1.repartition('id').join(df2.repartition('id'), 'id').show()\n",
    "\n",
    "#3. *Bucketing**: If the datasets are large, you can bucket them by the join key for more efficient joins.\n",
    "df1.write.bucketBy(10, 'id').saveAsTable('bucketed_table')\n",
    "\n",
    "#4.  **Predicate Pushdown**: Use predicates (filters) to reduce the size of the datasets before performing the join, thereby \n",
    "# reducing the shuffle size.\n",
    "df1.filter(df1['value'] > 100).join(df2, 'id').show()\n",
    "    \n",
    "\n",
    "#5. **Skewed Joins**: If one dataset has highly skewed keys (i.e., some keys occur much more often than others), \n",
    "# you can \"salt\" the join by adding random noise to the key to distribute the load.\n",
    "\n",
    "### Conclusion\n",
    "'''\n",
    "    Internally, PySparks joins are optimized based on the size and characteristics of the data, using strategies \n",
    "    like **sort-merge join**, **hash join**, and **broadcast join**. The decision to use shuffling, sorting, or broadcasting depends on \n",
    "    factors like data size and memory constraints, and it is important to understand how PySpark handles these operations to optimize your \n",
    "    joins effectively.\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
