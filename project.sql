
-- Project Understanding 
-- GLUE : ETL 
Q) What was the business problem you were solving, and why did you choose these specific AWS services?


-- 1. Source

Q) End to end flow 
Q) Data Size 
Q) What is data source 
A) Batch processing vs Real time streaming 
Q) How schema evoluation is handled

-- 2. Data Storage : Storage SIZE + COST 
   * Raw 
     Q) Type of Files 
     Q) Size of data 
     Q) Partition based on 
     Q) CURD Operation 

   * Refind 
     Q) Type of Files 
     Q) Size of data 
     Q) Partition based on 
     Q) CURD Operation 
   * 

-- 3. Data Transformation 


-- 4. Destination 
Q) What is data destination 


--- 5. Consumption 


Database => Redshift
======================
1. Cluster Size 
2. Running state => 24/7 
3. How do you connect to cluster ?
4. No of tables 
  * Dimensions Table 
  * Fact table 
6. Schema Type
6. Schema Evoution 


--- 3. INFRA SET 
Q) DEV/UAT/PROD  ENV
Q) tools used 

-- 4. Deployment 


-- 5. Testing 



#### Challenges Faced ########
in Spark 
1) Data Skewness
2) Large Dataset to JOIN 
3) Out of Memory 