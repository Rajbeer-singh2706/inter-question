{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project + AWS OR Databricks OR ANy TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empathetic, Understanding Others feelings , He was empathetic towards her situation.\n",
    "\n",
    "## compassionate, understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "\n",
    "# A) SQL Basics & Advanced:\n",
    "'''\n",
    "1. What are window functions in SQL? Provide examples of how they are used.\n",
    "3. How would you optimize a slow SQL query?\n",
    "3. Discuss the differences between OLAP and OLTP databases.\n",
    "4. Find out duplicates in a table?\n",
    "'''\n",
    "\n",
    "#2) Data Aggregation\n",
    "#Q) Given a sales(date, product_id,amount) , write a SQL query to calculate the 7-day rolling average of sales for each product.\n",
    "'''\n",
    "Q) To calculate the 7-day rolling average of sales for each product, you can use the `WINDOW` function with a range-based window specification. Here’s how you can write the SQL query to achieve this:\n",
    "\n",
    "SELECT\n",
    "    date,\n",
    "    product_id,\n",
    "    amount,\n",
    "    AVG(Amount) OVER (\n",
    "        PARTITION BY product_id\n",
    "        ORDER BY Date\n",
    "        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "    ) AS rolling_avg_7_days\n",
    "FROM\n",
    "    Sales\n",
    "ORDER BY\n",
    "    product_id,\n",
    "    date;\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **`SELECT Date, ProductID, Amount`**: This selects the `Date`, `ProductID`, and `Amount` columns from the `Sales` table.\n",
    "2. **`AVG(Amount) OVER (...) AS rolling_avg_7_days`**: The `AVG()` function calculates the average of the `Amount` values over a specified window of rows. The result is aliased as `rolling_avg_7_days`.\n",
    "3. **`PARTITION BY ProductID`**: This clause partitions the data by `ProductID`, so that the rolling average is calculated separately for each product.\n",
    "\n",
    "4. **`ORDER BY Date`**: This orders the data within each partition by `Date`, which is necessary for calculating the rolling average.\n",
    "\n",
    "5. **`ROWS BETWEEN 6 PRECEDING AND CURRENT ROW`**: This specifies the window frame for the rolling average calculation. It includes the current row and the 6 preceding rows, resulting in a 7-day window (including the current day).\n",
    "\n",
    "6. **`FROM Sales`**: This specifies the `Sales` table as the source of the data.\n",
    "\n",
    "7. **`ORDER BY ProductID, Date`**: This ensures the final result is ordered by `ProductID` and `Date`, making it easier to interpret the rolling averages over time.\n",
    "\n",
    "### Sample Data:\n",
    "\n",
    "| Date       | ProductID | Amount | rolling_avg_7_days |\n",
    "|------------|-----------|--------|--------------------|\n",
    "| 2024-08-01 | 1         | 100    | 100                |\n",
    "| 2024-08-02 | 1         | 150    | 125                |\n",
    "| 2024-08-03 | 1         | 200    | 150                |\n",
    "| 2024-08-04 | 1         | 250    | 175                |\n",
    "| 2024-08-05 | 1         | 300    | 200                |\n",
    "| 2024-08-06 | 1         | 350    | 225                |\n",
    "| 2024-08-07 | 1         | 400    | 250                |\n",
    "| 2024-08-08 | 1         | 450    | 300                |\n",
    "\n",
    "### Optimization Considerations:\n",
    "\n",
    "1. **Indexes**: \n",
    "   - Ensure there is an index on the `ProductID` and `Date` columns in the `Sales` table. This will help with the partitioning and ordering operations.\n",
    "\n",
    "2. **Data Size**:\n",
    "   - For very large datasets, consider partitioning the table by date ranges or using other techniques to manage large data volumes efficiently.\n",
    "\n",
    "3. **Query Execution Plan**:\n",
    "   - Check the execution plan to make sure that the query is using indexes appropriately and not performing full table scans or other costly operations.\n",
    "\n",
    "This query will compute the 7-day rolling average of sales for each product, considering the sales data over the specified window of time.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON THEORY\n",
    "'''\n",
    "    ➞ What are Python lists and how are they different from tuples?\n",
    "    ➞ How do you create a dictionary in Python and access its values?\n",
    "    ➞ Explain list comprehension and provide an example.\n",
    "    ➞ How can you read a CSV file in Python using pandas?\n",
    "    ➞ What is the difference between loc and iloc in pandas?\n",
    "    ➞ How do you handle missing data in a pandas DataFrame?\n",
    "    ➞ Explain the use of the apply() function in pandas.\n",
    "    ➞ How can you merge/join two DataFrames in pandas?Q) difference b/w merge or join \n",
    "    ➞ Describe how to group data in pandas and perform aggregation.\n",
    "    ➞ What are NumPy arrays and how do they differ from Python lists?\n",
    "    ➞ How do you perform element-wise operations on NumPy arrays?\n",
    "    ➞ What is the use of the Matplotlib library in Python? Provide an example of a simple plot.\n",
    "    ➞ How do you create subplots in Matplotlib?\n",
    "    ➞ Explain the use of the Seaborn library and provide an example of a categorical plot.\n",
    "    ➞ What is a lambda function in Python and how is it used?\n",
    "\n",
    "When practicing these Python questions, try using a small sample dataset like the Iris dataset. It's built into many Python libraries. This helps you see how these functions work with actual data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON PS\n",
    "'''\n",
    "## 1. Find the second largest number \n",
    "### 2.Group Anagrams\n",
    "Write a function that takes a list of strings and groups anagrams together.\n",
    "\n",
    "from collections import defaultdict\n",
    "def group_anagrams(strs):\n",
    "    anagrams = defaultdict(list)\n",
    "    for s in strs:\n",
    "        sorted_str = ''.join(sorted(s))\n",
    "        anagrams[sorted_str].append(s)\n",
    "    return list(anagrams.values())\n",
    "\n",
    "print(group_anagrams([\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"]))\n",
    "# [['eat', 'tea', 'ate'], ['tan', 'nat'], ['bat']]\n",
    "'''\n",
    "# ### 1. **Given an array of integers, find the pair of elements that sum up to a specific target.**\n",
    "# Example usage:\n",
    "nums = [2, 7, 11, 15]\n",
    "target = 9\n",
    "print(find_pair_with_sum(nums, target))  # Output: (0, 1) -> nums[0] + nums[1] = 2 + 7 = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYSPARK \n",
    "\n",
    "'''\n",
    " Q) How would you implement a custom transformation function in PySpark, and what are the potential performance implications?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here are some PySpark interview questions that are suitable for a junior data engineer:\n",
    "\n",
    "1. **What is PySpark, and how does it differ from Apache Spark?**\n",
    "   - **Answer:** PySpark is the Python API for Apache Spark, which allows users to interact with Spark using Python. It provides a way to perform distributed data processing and analytics using Python’s libraries and syntax, whereas Apache Spark is the core framework implemented in Scala.\n",
    "\n",
    "2. **What is a DataFrame in PySpark, and how do you create one?**\n",
    "   - **Answer:** A DataFrame in PySpark is a distributed collection of data organized into named columns. It can be created from a variety of sources such as existing RDDs, external data sources (e.g., JSON, CSV), or by converting a pandas DataFrame. Example of creating a DataFrame from a list:\n",
    "     ```python\n",
    "     from pyspark.sql import SparkSession\n",
    "     spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "     data = [(\"Alice\", 34), (\"Bob\", 45)]\n",
    "     df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "     ```\n",
    "\n",
    "3. **How do you perform basic data transformations in PySpark?**\n",
    "   - **Answer:** Basic data transformations include operations like filtering, selecting columns, and aggregating data. Examples:\n",
    "     ```python\n",
    "     df.filter(df.Age > 30).show()\n",
    "     df.select(\"Name\").show()\n",
    "     df.groupBy(\"Age\").count().show()\n",
    "     ```\n",
    "\n",
    "4. **What are some common functions used for data manipulation in PySpark?**\n",
    "   - **Answer:** Common functions include `select()`, `filter()`, `groupBy()`, `agg()`, `withColumn()`, and `drop()`. Example of adding a new column:\n",
    "     ```python\n",
    "     from pyspark.sql.functions import col\n",
    "     df = df.withColumn(\"AgeNextYear\", col(\"Age\") + 1)\n",
    "     ```\n",
    "\n",
    "5. **How do you handle missing or null values in PySpark?**\n",
    "   - **Answer:** Missing or null values can be handled using functions like `na.fill()`, `na.drop()`, or `fillna()`. Example:\n",
    "     ```python\n",
    "     df = df.na.fill({\"Age\": 0})\n",
    "     ```\n",
    "\n",
    "6. **Explain the concept of partitioning in PySpark and why it's important.**\n",
    "   - **Answer:** Partitioning refers to dividing a DataFrame into smaller, manageable pieces (partitions) that can be processed in parallel across a cluster. This improves performance and resource utilization. You can control partitioning using methods like `repartition()` or `coalesce()`.\n",
    "\n",
    "7. **What are RDDs in PySpark, and how are they related to DataFrames?**\n",
    "   - **Answer:** RDDs (Resilient Distributed Datasets) are the fundamental data structure in Spark, providing low-level operations. DataFrames are built on top of RDDs and offer a higher-level abstraction with optimizations and a schema.\n",
    "\n",
    "8. **How do you perform joins in PySpark?**\n",
    "   - **Answer:** Joins in PySpark are performed using methods like `join()`. Example of a basic join:\n",
    "     ```python\n",
    "     df1.join(df2, df1.id == df2.id, \"inner\").show()\n",
    "     ```\n",
    "\n",
    "9. **What is the `SparkSession` and why is it used?**\n",
    "   - **Answer:** `SparkSession` is the entry point for programming with Spark. It provides a unified interface for interacting with Spark and allows for the creation of DataFrames, execution of SQL queries, and configuration of Spark settings.\n",
    "\n",
    "10. **How do you optimize PySpark jobs?**\n",
    "    - **Answer:** Optimization techniques include caching intermediate DataFrames using `cache()` or `persist()`, tuning Spark configurations (e.g., `spark.sql.shuffle.partitions`), and avoiding wide transformations (e.g., `groupBy()`) when possible.\n",
    "\n",
    "These questions cover fundamental concepts and practical skills needed for a junior data engineer working with PySpark.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
