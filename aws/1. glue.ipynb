{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challanges \n",
    "'''\n",
    " - Glue Crawler runs periodcically to test new schema\n",
    " \n",
    "READING \n",
    " Q) How many tables reading from source \n",
    " Q) Complete data loading to GLUE \n",
    " Q) New changes Loaded , How to identify the new changes \n",
    "\n",
    "TRANSFORM \n",
    " Q) Cheking null\n",
    " Q) Data Cleansing \n",
    " Q) Date & Time Partining \n",
    " Q) \n",
    "\n",
    " \n",
    "# Layers : RAW => GLUE => CLENASED => COPY => REDSHIFT \n",
    "# Raw Data Size \n",
    "# Cleansed Data Size \n",
    "# COPY Data Size \n",
    "'''\n",
    "\n",
    "# How to clsuter is setup and worker are decide \n",
    "# What is data size , and cluster configuration or worker size ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Question \n",
    "'''\n",
    " - Cluster Size : Node \n",
    " - Data Size ( Volume)\n",
    " - Cost \n",
    " - How cluster is SETUP?\n",
    " - ENV [ DEV/UAT/PROD]\n",
    " - How many jobs , for example if there are 10 table how its processing 10 table\n",
    " Q) What is file format used ?\n",
    "\n",
    " Q) Have you done any auto-scaling \n",
    " Q) Batch vs Streaming\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **OPTIMIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Explain how you would optimize an AWS Glue ETL job for large datasets.\n",
    "'''\n",
    " To optimize an AWS Glue ETL job for large datasets, follow these best practices:\n",
    "\n",
    " - 1. Partitioning: Ensure your data is properly partitioned in S3 or other sources to allow for parallel processing.\n",
    " - 2. Memory Management: Use the appropriate worker types and number of workers for your job (Standard vs. G.1X or G.2X workers).\n",
    " - 3. Data Pruning: Use Glue Pushdown predicates to filter data at the source level to reduce data size.\n",
    " - 4. Job Bookmarking: Enable job bookmarking to process only new or modified data in incremental jobs.\n",
    " - 5. Parallel Processing: Ensure that your Glue job leverages parallelism by tuning the Spark configurations for memory, \n",
    " shuffle partitions, and concurrency.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Schema Evoution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Answer: AWS Glue provides the ability to handle schema evolution in several ways:\\n - DynamicFrames: Automatically handle missing or evolving fields using the `resolveChoice` method, allowing for flexible schema resolution.\\n - Data Catalog versioning: The Glue Data Catalog supports versioning, so schema changes can be tracked, and older versions \\n   can be restored if needed.\\n - Job-level logic: Implement logic in ETL jobs to account for schema evolution (e.g., handling new columns, renamed fields, or \\n    datatype changes) using custom transformations.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 9. How does AWS Glue handle schema changes?\n",
    "'''\n",
    " AWS Glue can handle schema changes using:\n",
    "   - DynamicFrames: Automatically resolve schema changes, including missing or new fields.\n",
    "   - Resolve Choice: A transformation method that helps handle situations where schema changes (such as data type modifications) occur.\n",
    "'''\n",
    "\n",
    "### 7. How can you handle schema changes in AWS Glue?\n",
    "'''\n",
    " Answer: AWS Glue provides the ability to handle schema evolution in several ways:\n",
    " - DynamicFrames: Automatically handle missing or evolving fields using the `resolveChoice` method, allowing for flexible schema resolution.\n",
    " - Data Catalog versioning: The Glue Data Catalog supports versioning, so schema changes can be tracked, and older versions \n",
    "   can be restored if needed.\n",
    " - Job-level logic: Implement logic in ETL jobs to account for schema evolution (e.g., handling new columns, renamed fields, or \n",
    "    datatype changes) using custom transformations.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Quality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to handle Data quality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Real-world Scenarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Advanced Topics\n",
    "'''\n",
    "    19. What are some common use cases for AWS Glue workflows, and how do they differ from jobs?\n",
    "    20. How do you handle partitioning in AWS Glue to optimize large data processing?\n",
    "    21. What are the limitations of AWS Glue, and how do you work around them?\n",
    "    22. Describe a complex ETL pipeline youve built using AWS Glue. What were the challenges, and how did you overcome them?\n",
    "    23. How would you migrate an on-premises ETL workflow to AWS Glue?\n",
    "    24. Can you discuss a scenario where you had to optimize an AWS Glue job for cost-efficiency?\n",
    "'''\n",
    "\n",
    "### 11. Can AWS Glue integrate with other AWS services, and if so, how?\n",
    "'''\n",
    "   AWS Glue integrates seamlessly with various AWS services:\n",
    "   - Amazon S3: For data storage and input/output for ETL jobs.\n",
    "   - AWS Athena: You can query Glue Catalog data using Athena for serverless queries.\n",
    "   - Amazon Redshift: Load transformed data into Redshift for analysis.\n",
    "   - Amazon RDS & DynamoDB: Glue supports JDBC connections to relational databases and DynamoDB.\n",
    "   - AWS Lambda: Trigger Glue jobs based on custom events from AWS Lambda functions.\n",
    "   - AWS Kinesis: Real-time streaming data can be processed with Glue jobs triggered from Kinesis Data Streams.\n",
    "'''\n",
    "\n",
    "### 10. How would you secure sensitive data in AWS Glue?\n",
    "'''\n",
    "   Answer: To secure sensitive data in AWS Glue:\n",
    "   - IAM Policies: Use fine-grained IAM policies to control access to Glue jobs, crawlers, and the Data Catalog.\n",
    "   - Data Encryption: Enable encryption at rest for data stored in S3 and encrypt data in transit using SSL.\n",
    "   - Connection Encryption: Use JDBC connections with SSL/TLS for secure access to databases.\n",
    "   - Secrets Manager: Store and retrieve sensitive connection details, such as database credentials, using AWS Secrets Manager.\n",
    "   - Network Security: Use VPC endpoints to ensure Glue jobs run securely without exposing them to the public internet\n",
    "'''\n",
    "\n",
    "### Integration with Other AWS Services\n",
    "'''\n",
    "16. How does AWS Glue integrate with AWS S3, and what are some best practices for this integration?\n",
    "17. Can you explain how AWS Glue can be used with Amazon Athena for querying data?\n",
    "18. Describe a scenario where you would use AWS Glue in combination with AWS Lambda.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. What is AWS Glue?\n",
    "'''\n",
    "AWS Glue is a fully managed ETL (Extract, Transform, Load) service that helps prepare and transform data for analytics. \n",
    "It automates the process of discovering, cleaning, enriching, and moving data between different data stores like S3, Redshift, and RDS.\n",
    "\n",
    "'''\n",
    "### 1. What is AWS Glue, and what are its primary components?\n",
    "'''\n",
    " AWS Glue is a fully managed ETL (Extract, Transform, Load) service that makes it easy to prepare and load data for analytics. \n",
    "     Its primary components are:\n",
    "   - Data Catalog: A central repository to store metadata about data sources, such as databases, tables, and columns.\n",
    "   - Crawlers: Used to discover data in sources and infer its schema.\n",
    "   - ETL Jobs: Python or Scala scripts that extract, transform, and load data between different data stores.\n",
    "   - Triggers: Used to orchestrate ETL workflows by triggering jobs based on schedules or events.\n",
    "   - Workflows: Enable the orchestration of complex ETL processes, connecting jobs and triggers.\n",
    "'''\n",
    "\n",
    "### 2. What are the primary use cases of AWS Glue?\n",
    "#AWS Glue is used for:\n",
    "'''\n",
    "   - Data preparation and transformation for analytics.\n",
    "   - Building ETL pipelines to move data between storage systems (e.g., S3 to Redshift).\n",
    "   - Cleaning and normalizing raw data for further analysis.\n",
    "   - Integrating data from various sources into a central data warehouse.\n",
    "'''\n",
    "\n",
    "### Glue\n",
    "'''\n",
    "1. How would you use AWS Glue to automate ETL workflows for a large dataset?\n",
    "2. Can you explain the difference between Glue Jobs, Crawlers, and Data Catalog?\n",
    "3. Describe how AWS Glue integrates with other AWS services like S3, Athena, and Redshift.\n",
    "4. How would you handle schema evolution in AWS Glue when dealing with semi-structured data?\n",
    "5. Can you explain the architecture of AWS Glue and how it works with ETL jobs?\n",
    "6. What are the main components of AWS Glue, and how do they interact?\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Glue Data Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. What is the AWS Glue Data Catalog?\n",
    "'''\n",
    "The AWS Glue Data Catalog is a central metadata repository where information about data, such as its schema, format, \n",
    "and location, is stored. \n",
    "It helps with discovering and organizing datasets across various sources and allows easy querying using services like Amazon Athena.\n",
    "'''\n",
    "\n",
    "### 3. What is the AWS Glue Data Catalog, and why is it important?\n",
    "'''\n",
    " The AWS Glue Data Catalog is a metadata repository where you store information about your data such as its \n",
    " schema, format, location, and partitions. \n",
    " It is important because it serves as the foundation for managing and querying data in AWS Glue, allowing users \n",
    "to easily track and discover datasets, integrate with other AWS services (such as Athena and Redshift), and manage schema versions.\n",
    "'''\n",
    "\n",
    "### Data Catalog and Crawlers\n",
    "'''\n",
    "    4. What is the AWS Glue Data Catalog, and how does it help manage metadata?\n",
    "    5. How do Glue Crawlers work, and when would you use them?\n",
    "    6. Can you describe how to handle schema evolution in AWS Glue?\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. What is an AWS Glue Crawler?\n",
    "'''\n",
    " A Glue Crawler is a component that automatically discovers data stored in various sources like Amazon S3 or relational databases. \n",
    " It determines the datas schema, and metadata is stored in the Glue Data Catalog. This makes it easier to create ETL jobs.\n",
    "'''\n",
    "### 2. How does AWS Glue Crawlers work?\n",
    "'''\n",
    " Crawlers in AWS Glue connect to data sources (like S3, JDBC, or DynamoDB), traverse them, and extract metadata to create or \n",
    " update tables in the Glue Data Catalog. Crawlers can infer schema by reading data from the source and then storing the metadata information \n",
    " such as data format, partitioning, and data types.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Glue DynamicFrames and DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. What is the difference between Glue DynamicFrames and DataFrames?\n",
    "'''\n",
    "   - DynamicFrames: \n",
    "      * These are AWS Glue-specific abstractions based on Apache Spark DataFrames. \n",
    "      * They allow you to manage data with more flexibility, especially when working with semi-structured data like JSON. \n",
    "      * They automatically handle schema changes.\n",
    "      *  They are an AWS Glue-specific abstraction on top of Apache Spark DataFrames that offer additional ETL functionality,\n",
    "        such as automatic schema resolution and transformation. \n",
    "      * DynamicFrames can handle semi-structured data (e.g., JSON) more easily and support transformations \n",
    "    like `applyMapping`, `resolveChoice`, and `unbox`.\n",
    "\n",
    "    \n",
    "   - DataFrames: \n",
    "        * Spark's native abstraction, ideal for structured data where schema consistency is important.\n",
    "        * These are a core Spark abstraction and are more performant for structured data processing. DataFrames are strictly \n",
    "   schema-bound and are faster for queries, but they dont have the flexibility for complex transformations like DynamicFrames.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. How can you create an AWS Glue job?\n",
    "'''\n",
    "   Answer: To create an AWS Glue job:\n",
    "   1. Define the source and target data stores (e.g., S3, Redshift).\n",
    "   2. Create a job script (in Python or Scala) to transform the data.\n",
    "   3. Specify the data source, apply transformations, and load the data into the target location.\n",
    "   4. Schedule or trigger the job to run manually or at set intervals.\n",
    "'''\n",
    "\n",
    "### 11. What are some common errors you might encounter while using AWS Glue, and how do you resolve them?\n",
    "'''\n",
    " Ans) Common errors include:\n",
    "   - Out of memory: Adjust job worker size (G.1X or G.2X) or increase the number of workers.\n",
    "   - Schema mismatch: Use DynamicFrames and apply appropriate transformations (e.g., `resolveChoice`).\n",
    "   - Timeout: Increase the timeout settings for the job or optimize the ETL script for better performance.\n",
    "'''\n",
    "### 9. How do you debug AWS Glue jobs?\n",
    "'''\n",
    "Ans) Debugging AWS Glue jobs can be done through\n",
    "   - Logs: AWS Glue integrates with CloudWatch Logs, where you can inspect job execution logs for errors and warnings.\n",
    "   - Job Monitoring: Use the Glue job monitoring interface to track job progress, resource consumption, and errors.\n",
    "   - Development Endpoints: You can create a development endpoint to interactively test and debug your ETL scripts using notebooks \n",
    "   like SageMaker or Zeppelin.\n",
    "   - Error Handling: Implement error handling using try-except blocks in your Python scripts to capture and log exceptions.\n",
    "'''\n",
    "\n",
    "### ETL Jobs and Transformation\n",
    "'''\n",
    " 7. How would you create and manage an ETL job in AWS Glue using PySpark?\n",
    " 9. How would you handle complex data transformations in AWS Glue?\n",
    " 10. How can you optimize the performance of a Glue ETL job?\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. What are some ways to trigger an AWS Glue job?\n",
    "'''\n",
    " You can trigger an AWS Glue job in different ways:\n",
    "   - On-demand: Manually starting the job.\n",
    "   - Time-based triggers: Scheduling the job to run at regular intervals.\n",
    "   - Event-based triggers: Trigger the job based on events, such as a new file being added to an S3 bucket.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. What is partitioning in AWS Glue, and why is it important?\n",
    "'''\n",
    " - Partitioning is a way of dividing large datasets into smaller, more manageable pieces, often based on a key column \n",
    "like date or location. \n",
    "- In AWS Glue, partitioning improves query performance by only scanning the relevant portions of data, reducing the time \n",
    "and cost of processing.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Job bookmarking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 10. What is job bookmarking in AWS Glue?\n",
    "'''\n",
    "- Job bookmarking is a feature that tracks previously processed data. When enabled, it ensures that the AWS Glue job processes only \n",
    "   new or updated data during subsequent runs. \n",
    "- This helps with incremental data processing and avoids reprocessing the entire dataset.\n",
    "'''\n",
    "### 6. What is AWS Glue job bookmarking, and why is it useful?\n",
    "'''\n",
    " - AWS Glue job bookmarking tracks previously processed data and ensures that your ETL job only processes new or modified data \n",
    "    in subsequent runs. \n",
    " - This is useful for incremental processing, saving both time and resources by avoiding redundant data extraction and \n",
    " transformation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. What is the purpose of AWS Glue Workflows, and how do you use them?\n",
    "'''\n",
    "   Answer: AWS Glue Workflows allow you to create a sequence of interconnected ETL jobs and triggers to build a complex ETL pipeline. \n",
    "   With workflows, you can schedule and monitor multiple Glue jobs in a single pipeline. You can orchestrate the flow of execution, \n",
    "   manage dependencies, and integrate different components like triggers (time-based or event-based) and crawlers.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security and Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " 11. How do you ensure data security and compliance when using AWS Glue?\n",
    " 12. What IAM roles and policies are required for running AWS Glue jobs?\n",
    " 13. How do you encrypt data at rest and in transit in AWS Glue?\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling and Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " 14. What strategies do you use to debug and troubleshoot AWS Glue jobs?\n",
    " 15. How would you handle failed jobs in AWS Glue, and what retry strategies can be implemented?\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
