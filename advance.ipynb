{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project + AWS OR Databricks OR ANy TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL \n",
    "# Q) Write a SQL query to pivot a table, turning rows into columns.\n",
    "'''\n",
    "To pivot a table in SQL Server, you can use the `PIVOT` operator. This operator allows you to convert rows into columns, typically used for summarizing data.\n",
    "\n",
    "### Example Scenario:\n",
    "Suppose you have a table named `Sales` with the following structure:\n",
    "\n",
    "| SalesPerson | Product  | Amount |\n",
    "|-------------|----------|--------|\n",
    "| Alice       | ProductA | 100    |\n",
    "| Alice       | ProductB | 150    |\n",
    "| Bob         | ProductA | 120    |\n",
    "| Bob         | ProductC | 130    |\n",
    "\n",
    "You want to pivot this table to show the total sales amount by each salesperson for each product.\n",
    "\n",
    "### Pivot Query:\n",
    "\n",
    "```sql\n",
    "SELECT SalesPerson, \n",
    "       [ProductA], \n",
    "       [ProductB], \n",
    "       [ProductC]\n",
    "FROM \n",
    "(\n",
    "    SELECT SalesPerson, Product, Amount\n",
    "    FROM Sales\n",
    ") AS SourceTable\n",
    "PIVOT\n",
    "(\n",
    "    SUM(Amount)\n",
    "    FOR Product IN ([ProductA], [ProductB], [ProductC])\n",
    ") AS PivotTable;\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- The inner query (`SourceTable`) selects the `SalesPerson`, `Product`, and `Amount` columns.\n",
    "- The `PIVOT` operator aggregates the `Amount` values by `SalesPerson` for each `Product`.\n",
    "- The `FOR Product IN ([ProductA], [ProductB], [ProductC])` clause specifies the values from the `Product` column that should become new columns.\n",
    "\n",
    "### Result:\n",
    "\n",
    "| SalesPerson | ProductA | ProductB | ProductC |\n",
    "|-------------|----------|----------|----------|\n",
    "| Alice       | 100      | 150      | NULL     |\n",
    "| Bob         | 120      | NULL     | 130      |\n",
    "\n",
    "This query transforms the rows into columns based on the `Product` values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PYTHON\n",
    "# 1. Find the Longest Substring Without Repeating Characters\n",
    "'''\n",
    "Problem: Given a string, find the length of the longest substring without repeating characters.\n",
    "\n",
    "Input: \"abcabcbb\"\n",
    "Output: 3 (The answer is \"abc\", with a length of 3.)\n",
    "Hint: Use a sliding window approach to solve this problem efficiently.\n",
    "'''\n",
    "\n",
    " \n",
    "### 2. **Implement a Python script that reads a large CSV file in chunks and processes it.**\n",
    "import pandas as pd\n",
    "\n",
    "def process_csv_in_chunks(file_path, chunk_size=10000):\n",
    "    chunk_iter = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "    \n",
    "    for chunk in chunk_iter:\n",
    "        # Process the chunk\n",
    "        process_chunk(chunk)\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    # Example processing: Print the first 5 rows of each chunk\n",
    "    print(chunk.head())\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'large_file.csv' with the path to your large CSV file\n",
    "file_path = 'large_file.csv'\n",
    "process_csv_in_chunks(file_path, chunk_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PYSPARK \n",
    "'''\n",
    "\n",
    "Advanced Transformations and Actions:\n",
    "\n",
    "Explain the difference between map and flatMap in PySpark. When would you use one over the other?\n",
    "How would you implement a custom transformation function in PySpark, and what are the potential performance implications?\n",
    "\n",
    "Optimization Techniques:\n",
    "Q1) What are some common techniques for optimizing Spark jobs? Discuss strategies for tuning Spark performance, including partitioning, caching, and memory management.\n",
    "Q) How do you handle skewed data in PySpark, and what strategies can you use to mitigate its impact?\n",
    "\n",
    "Data Aggregation and Join Operations:\n",
    "Q1) How would you perform a large-scale join operation between two DataFrames efficiently? What considerations do you need to take into account for performance?\n",
    "Q2) Describe how you would implement a rolling window function in PySpark. What are the performance implications of using window functions?\n",
    "\n",
    "Error Handling and Debugging:\n",
    "Q1) Describe a situation where you encountered a performance bottleneck or error in a PySpark job. How did you diagnose and resolve the issue?\n",
    "Q2) How do you debug and handle exceptions in PySpark applications? What tools and techniques do you use for effective debugging?\n",
    "\n",
    "Integration with Other Systems:\n",
    "Q1) How do you integrate PySpark with other data systems or tools, such as AWS services or databases? Provide examples of how youâ€™ve used PySpark in a larger data pipeline.\n",
    "Q2) Discuss how you would use PySpark with a streaming data source. What considerations are necessary for ensuring data consistency and fault tolerance?\n",
    "Complex Use Cases:\n",
    "\n",
    "Explain how you would approach implementing a machine learning pipeline using PySpark. What steps would you include, and how would you ensure scalability and performance?\n",
    "Describe a challenging data engineering problem you solved with PySpark. What were the key factors in developing a successful solution?\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
