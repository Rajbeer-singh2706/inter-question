{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEDIUM \n",
    "#In Spark and Big Data ecosystems, several file formats are commonly used for data storage and processing, \n",
    "#each with unique characteristics in terms of performance, compression, and schema support. Here are the key file formats:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Common File Formats in Spark and Big Data:\n",
    "\n",
    "##1. CSV (Comma-Separated Values)\n",
    "'''\n",
    "   - Characteristics: Text-based, simple, human-readable.\n",
    "   - Advantages: Easy to read and write, compatible with many tools.\n",
    "   - Disadvantages: No schema support, larger file sizes, slower processing.\n",
    "'''\n",
    "\n",
    "## 2. JSON (JavaScript Object Notation)\n",
    "'''\n",
    "   - Characteristics: Text-based, semi-structured, schema-free.\n",
    "   - Advantages: Human-readable, flexible.\n",
    "   - Disadvantages: Larger size compared to binary formats, not ideal for large datasets.\n",
    "'''\n",
    "\n",
    "## 3. Parquet\n",
    "'''\n",
    "   - Characteristics: Columnar, binary format.\n",
    "   - Advantages: Highly efficient for analytical queries, supports schema evolution, good compression.\n",
    "   - Disadvantages: More complex to handle compared to CSV.\n",
    "'''\n",
    "\n",
    "## 4. Avro\n",
    "'''\n",
    "   - Characteristics: Row-based, binary format.\n",
    "   - Advantages: Supports schema evolution, compact, good for streaming.\n",
    "   - Disadvantages: Slower analytical performance compared to columnar formats like Parquet.\n",
    "'''\n",
    "\n",
    "## 5. ORC (Optimized Row Columnar)\n",
    "'''\n",
    "   - Characteristics: Columnar, binary format.\n",
    "   - Advantages: Optimized for Hive, better compression than Parquet, schema evolution support.\n",
    "   - Disadvantages: Complex to manage in non-Hive environments.\n",
    "\n",
    "'''\n",
    "\n",
    "## 6. Delta Lake\n",
    "'''\n",
    "   - Characteristics: Layer on top of Parquet, ACID transactions, supports schema enforcement.\n",
    "   - Advantages: Handles batch and streaming data, supports versioning and time travel.\n",
    "   - Disadvantages: Newer technology, limited to Spark environments.\n",
    "'''\n",
    "\n",
    "## 7. SequenceFile\n",
    "'''\n",
    "   - Characteristics: Hadoop-specific format, binary key-value pairs.\n",
    "   - Advantages: Efficient for large key-value data sets.\n",
    "   - Disadvantages: Less flexible for non-Hadoop ecosystems.\n",
    "'''\n",
    "\n",
    "### Interview Questions for Data Engineer (on File Formats and Big Data):\n",
    "'''\n",
    "   1. What are the advantages and disadvantages of using Parquet over CSV in Spark?\n",
    "   **2. How does schema evolution work in Avro and Parquet, and when would you choose one over the other?\n",
    "   3. Explain the difference between row-based and columnar file formats.\n",
    "   4. In what scenarios would you choose JSON over Parquet or ORC?\n",
    "   **5. What are the benefits of using Delta Lake over traditional Parquet or ORC?\n",
    "   6. How does compression work in Spark for different file formats, and which formats are best suited for compression?\n",
    "   7. Can you explain how Spark handles reading and writing different file formats?\n",
    "   8. What is the impact of using different file formats on Spark's partitioning strategy?\n",
    "   \n",
    "   ** 9. What file format would you recommend for storing time-series data in a Big Data environment? Why?\n",
    "   10. How do you optimize read performance in Spark when dealing with large Parquet files?\n",
    "   11. What are the key differences between ORC and Parquet in terms of storage and query performance?\n",
    "   12. How does file format selection impact performance in distributed computing systems like Hadoop or Spark?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typically go deeper into optimization strategies, data architecture, and decisions around file formats for\n",
    "#specific workloads. Here are some advanced interview questions related to file formats and Big Data:\n",
    "'''\n",
    "1. When designing a data lake, how do you decide which file format to use (e.g., Parquet, ORC, Avro, JSON, CSV)? \n",
    "Discuss trade-offs and performance considerations.\n",
    "\n",
    "2. Explain the internals of columnar storage in Parquet or ORC and how it improves query performance in Spark compared to row-based\n",
    "formats like CSV or JSON.\n",
    "\n",
    "3. How does predicate pushdown work in columnar formats like Parquet and ORC, and how does it enhance query efficiency in Spark?\n",
    "\n",
    "4. Describe how file format choice affects partitioning strategies in Spark and how you would optimize data layout for performance \n",
    "in large-scale ETL jobs.\n",
    "\n",
    "5. What are the challenges of using Avro for streaming data pipelines, and how would you address schema evolution in such a pipeline?\n",
    "\n",
    "6. Explain the role of file formats in data compression and how you would optimize compression and decompression performance \n",
    "in a Big Data environment.\n",
    "\n",
    "7. How do ACID guarantees in Delta Lake differ from other file formats like Parquet or ORC, and how would you architect a solution \n",
    "to leverage these features?\n",
    "\n",
    "8. When dealing with multi-terabyte datasets, how would you architect a solution to minimize small file issues, \n",
    "especially when working with formats like Parquet or ORC?\n",
    "\n",
    "9. Describe a real-world use case where the choice of file format significantly impacted performance and how you optimized it.\n",
    "\n",
    "10. What are the best practices for handling large-scale schema changes in Big Data environments where Parquet, ORC, or Avro files are used?\n",
    "\n",
    "11. How would you implement data versioning in a data lake using Delta Lake or Apache Iceberg, and what are the key considerations?\n",
    "\n",
    "12. Can you explain how file format selection affects Sparkâ€™s Catalyst optimizer, and how would you structure data to maximize \n",
    "query performance?\n",
    "\n",
    "13. How would you handle a scenario where you need to migrate a dataset from JSON to a columnar format like Parquet, \n",
    "ensuring minimal disruption to downstream systems?\n",
    "\n",
    "14. What strategies would you employ to balance read and write performance when working with Parquet files in a distributed\n",
    "data processing environment?\n",
    "\n",
    "15. How do data lakes built on open formats like Delta Lake or Apache Iceberg differ from traditional HDFS-based architectures\n",
    "in terms of file format management and optimization?\n",
    "\n",
    "16. Explain the importance of partition pruning in columnar formats and how it can be leveraged\n",
    "for optimizing queries in Spark.\n",
    "\n",
    "17. What are the key factors to consider when selecting a file format for high-frequency streaming data\n",
    "versus large-scale batch processing?\n",
    "\n",
    "18. How do you handle late-arriving data in file formats like Delta Lake or Apache Hudi,\n",
    "ensuring consistency and efficiency?\n",
    "\n",
    "19. Describe a scenario where you had to troubleshoot performance issues in a Spark job \n",
    "due to file format or schema-related problems. How did you resolve it?\n",
    "\n",
    "20. Discuss the role of metadata management in Parquet or ORC and how it can affect query \n",
    "performance and data governance in a large-scale data environment.\n",
    "\n",
    "'''\n",
    "\n",
    "# These questions are aimed at evaluating advanced understanding, architecture decision-making, and performance optimization in Big Data systems using different file formats."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
